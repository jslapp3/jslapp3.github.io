{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Portfolio!","text":"<p>Hello, my name is Jacob Lapp, and I am a current Master of Data Science student at Rice University.  Upon graduating in December 2024, I am open to work!</p> <p>Resume</p>"},{"location":"#center-for-research-computing","title":"Center for Research Computing","text":"<ul> <li>HPC and other Technical Documentation</li> <li>Commission Shift Collaboration<ul> <li>Project README</li> <li>StoryMap Link</li> </ul> </li> </ul>"},{"location":"#projects","title":"Projects","text":"<ul> <li>Rice Machine Learning Course Final Project<ul> <li>Project Poster</li> <li>Project Paper</li> <li>Main Notebook</li> </ul> </li> <li>Rice Information Retrieval Final Project<ul> <li>Project Paper</li> <li>Video Link</li> </ul> </li> <li>Rice Computer Ethics Final Paper<ul> <li>Final Paper</li> </ul> </li> </ul>"},{"location":"#science-for-georgia","title":"Science for Georgia","text":"<ul> <li>StoryMap Link </li> <li>Environmental Justice Index Notebook</li> </ul>"},{"location":"#emory-writings","title":"Emory Writings","text":"<ul> <li>History of Urban Crises Final Paper</li> <li>Public Policy Analysis Final Paper</li> <li>Election Analytics Paper</li> </ul>"},{"location":"crc/","title":"Spatial Studies Lab x Commission Shift","text":""},{"location":"crc/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Directory Structure</li> <li>Setup</li> <li>Usage</li> <li>Scripts</li> <li>Dependencies</li> <li>Contributing</li> <li>License</li> </ol>"},{"location":"crc/#overview","title":"Overview","text":"<p>This project downloads and processes various oil, gas, and water data across Texas. It includes functionality for consolidating datasets including pipelines, wells, and other related information into shapefiles and other data structures using Python scripts.</p>"},{"location":"crc/#directory-structure","title":"Directory Structure","text":"<ul> <li><code>code</code>: Contains Python scripts for various data processing tasks.</li> <li><code>data</code>: Stores data files used or generated by the scripts. (not included in git)</li> <li><code>tests</code>: Contains near-duplicated Python scripts from <code>code</code>, with slight modifications.</li> <li><code>requirements.txt</code>: Lists Python dependencies required to run the project.</li> </ul>"},{"location":"crc/#setup","title":"Setup","text":"<p>To set up this project locally, follow these steps:</p> <ol> <li>Clone the repository:</li> </ol> <p><code>bash    git clone https://github.com/jslapp3/CCSUs.git    cd CCSUs</code></p> <ol> <li> <p>Install dependencies:</p> <p><code>bash pip install -r requirements.txt</code></p> </li> <li> <p>Test the code</p> <p>If you would like to watch the code run on a small subset of all the data, run the main test script. To run all the tests at once, execute the test_main.py script located within the tests/test_code/ directory:</p> </li> </ol> <p><code>bash    python3 tests/test_code/test_main.py</code></p> <pre><code>Note: It is **not** necessary to run the tests before running the main script.\n</code></pre>"},{"location":"crc/#usage","title":"Usage","text":"<p>To run the main script and process the data, use the following command:</p> <p><code>bash    python code/main.py &lt;root_directory&gt;</code></p> <p>Replace  with the path to your root directory where data will be stored.  <p>However, the argument is optional. If main.py is run without specifying, the scripts default to the current directory. If your current directory is a suitable location, then just run:</p> <p><code>bash    python code/main.py</code></p>"},{"location":"crc/#scripts","title":"Scripts","text":"<p><code>Main Scripts</code></p> <ul> <li><code>main.py</code>: The main entry point for the project. It coordinates data processing tasks by calling functions from various modules.</li> </ul> <p><code>Subdirectory Scripts</code></p> <ul> <li> <p><code>Historical Data</code></p> <ul> <li><code>nrc_pipeline.py</code>: Fetches and downloads historical data from the National Response Center</li> <li><code>nrc.py</code>: Processes and merges historical data from the NRC</li> </ul> </li> <li> <p><code>Inactive Wells</code></p> <ul> <li><code>inactive_pipeline.py</code>: Downloads and processes inactive pipeline data from the Texas Railroad Commission</li> </ul> </li> <li> <p><code>Orphan Wells</code></p> <ul> <li><code>orphan_pipeline.py</code>:  Downloads and processes orphan pipeline data from the Texas Railroad Commission</li> </ul> </li> <li> <p><code>RRC Pipelines Wells</code></p> <ul> <li><code>pipelines_pipeline.py</code>: Downloads pipeline data from the Texas Railroad Commission</li> <li><code>merge_pipelines.py</code>: Merges the downloaded pipeline data from the Texas Railroad Commission</li> <li><code>wells_pipeline.py</code>: Downloads well data from the Texas Railroad Commission</li> <li><code>merge_wells.py</code>: Merges the downloaded well data from the Texas Railroad Commission</li> </ul> </li> <li> <p><code>Water Wells</code></p> <ul> <li><code>brac_locs.py</code>: Downloads BRACS well locations data from the Texas Water Development Board.</li> <li><code>groundwater_wells.py</code>: Downloads groundwater wells data from the Texas Water Development Board.</li> <li><code>sdrdb_wells.py</code>: Downloads SDRDB wells data from the Texas Water Development Board.</li> <li><code>move_water_wells.py</code>: Organizes and moves water wells data from the Texas Water Development Board.</li> </ul> </li> </ul>"},{"location":"crc/#dependencies","title":"Dependencies","text":"<p>You need Anaconda (or some way to run Python code) installed on your machine. To run, ensure you have the required packages by installing from requirements.txt (command above).</p>"},{"location":"crc/#contributing","title":"Contributing","text":"<p>If you would like to contribute to this project, please fork the repository and create a pull request with your changes. Ensure that all code adheres to the project's coding standards and passes all tests.</p>"},{"location":"crc/#credits","title":"Credits","text":"<p>This project was made possible by the support of the Diluvial Grant, in partnership with Commission Shift, the Center for Research Computing/Spatial Studies Lab, and Gulf Scholars.</p>"},{"location":"crc/#license","title":"License","text":"<p>This project is licensed under </p> <p> </p>"},{"location":"crc/#disclaimer","title":"Disclaimer","text":"<p>Our website provides information and data compiled from multiple public sources. While we strive to ensure the accuracy and reliability of the data, we cannot guarantee its completeness, timeliness, or accuracy. The information on this website is provided \"as is\" without any warranties of any kind, express or implied. The data and information provided are for general informational purposes only and are not intended to be a substitute for professional advice. Always seek the advice of a qualified professional with any questions you may have regarding a specific issue. Our website may contain links to third-party websites, and we are not responsible for the content, accuracy, or opinions expressed in such websites. Inclusion of any linked website on our site does not imply approval or endorsement by us. In no event shall our website, its owners, affiliates, or contributors be liable for any direct, indirect, incidental, special, or consequential damages, or damages of any kind, including but not limited to, loss of use, data, or profits, arising out of or in connection with the use of this website or the data contained herein. We may update this Disclaimer from time to time by posting the new Disclaimer on this page. Changes are effective when posted. If you have any questions, please contact us at ppowell@commissionshift.org.</p>"},{"location":"final_ml_project/pitch_nn/","title":"More Data Prep","text":"In\u00a0[2]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\nimport tensorflow as tf\n\n\nimport keras\nimport keras.callbacks\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 100)\n</pre> import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from matplotlib.patches import Rectangle  import tensorflow as tf   import keras import keras.callbacks from keras import optimizers from keras import backend as K from keras.layers import Dense from keras.models import Sequential  from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split  pd.set_option('display.max_columns', 200) pd.set_option('display.max_rows', 100) <p>NOT DONE WITH DATA PREP WHEN REMOVING ROWS I ALSO HAVE TO REMOVE ROWS ASSOCIATED WITH SCREWBALLS AND WPS</p> In\u00a0[3]: Copied! <pre>cols = ['sequence1_id', 'sequence2_id', 'pitch_value', 'linear_weight', 'bat_side', 'pitch_hand',\n        'pitch_type',\t'ax',\t'ay',\t'az',\t'vx0',\t'vy0',\t'vz0', 'x0', 'z0',\t'extension',\n        'strike_zone_top', 'strike_zone_bottom',\n        'cy',\t't0',\t'bx',\t'by',\t'bz',\t'cx',\t'cz',\t'release_x', 'release_y',\t'release_z',\n        'plate_y',\t'plate_time',\t'plate_x',\t'plate_z',\t'plate_x_line','plate_z_line',\n        'plate_z_gravity',\t'horz_break',\t'vert_break','induced_vert_break',\t'release_speed']\n\nseqs = pd.read_csv('sequences.csv', usecols=cols)\nseqs = seqs.reindex(columns=cols)\n</pre> cols = ['sequence1_id', 'sequence2_id', 'pitch_value', 'linear_weight', 'bat_side', 'pitch_hand',         'pitch_type',\t'ax',\t'ay',\t'az',\t'vx0',\t'vy0',\t'vz0', 'x0', 'z0',\t'extension',         'strike_zone_top', 'strike_zone_bottom',         'cy',\t't0',\t'bx',\t'by',\t'bz',\t'cx',\t'cz',\t'release_x', 'release_y',\t'release_z',         'plate_y',\t'plate_time',\t'plate_x',\t'plate_z',\t'plate_x_line','plate_z_line',         'plate_z_gravity',\t'horz_break',\t'vert_break','induced_vert_break',\t'release_speed']  seqs = pd.read_csv('sequences.csv', usecols=cols) seqs = seqs.reindex(columns=cols) In\u00a0[58]: Copied! <pre>seqs.head()\n</pre> seqs.head() Out[58]: sequence1_id sequence2_id pitch_value linear_weight bat_side pitch_hand pitch_type ax ay az vx0 vy0 vz0 x0 z0 extension strike_zone_top strike_zone_bottom cy t0 bx by bz cx cz release_x release_y release_z plate_y plate_time plate_x plate_z plate_x_line plate_z_line plate_z_gravity horz_break vert_break induced_vert_break release_speed 0 NaN 1.0 -0.041983 0.474798 R R FF -4.172672 24.453799 -15.239761 4.225669 -130.507625 -5.021063 -1.169861 6.025868 5.995655 3.28 1.62 54.504345 0.034403 4.369223 -131.348912 -4.496768 -1.317707 6.189589 -1.317707 54.504345 6.189589 1.416667 0.420644 0.151021 2.949780 0.520180 4.298051 1.451950 -4.429909 -16.179259 17.973953 131.498471 1 1.0 2.0 -0.055163 0.474798 R R SI -14.149811 24.987614 -22.900697 5.985844 -130.867315 -5.029194 -1.227174 5.946926 6.031877 3.28 1.56 54.468123 0.034032 6.467387 -131.717690 -4.249841 -1.439077 6.104817 -1.439077 54.468123 6.104817 1.416667 0.419455 0.028925 2.307596 1.273702 4.322200 1.492164 -14.937321 -24.175240 9.785187 131.944829 2 2.0 3.0 0.000000 0.474798 R R SI -13.640156 23.055409 -25.434018 4.267894 -132.395371 -4.037954 -1.004108 6.021157 6.178539 3.28 1.56 54.321461 0.032548 4.711859 -133.145786 -3.210119 -1.150246 6.139113 -1.150246 54.321461 6.139113 1.416667 0.412044 -0.366668 2.657300 0.791249 4.816401 2.085481 -13.895001 -25.909214 6.861833 133.267801 3 3.0 4.0 0.000000 0.474798 R R FC 2.864789 20.046111 -30.204211 4.261071 -123.300332 -4.764618 -0.925353 6.174098 6.281141 3.28 1.56 54.218859 0.034121 4.163321 -123.984335 -3.734006 -1.069080 6.319091 -1.069080 54.218859 6.319091 1.416667 0.441646 1.049025 1.724296 0.769635 4.669982 1.532582 3.352685 -35.348227 2.300578 124.110400 4 4.0 5.0 0.018436 0.474798 R R SI -17.457082 26.013916 -21.608071 8.177901 -133.567058 -8.057398 -0.901355 5.911378 6.169876 3.28 1.56 54.330124 0.032317 8.742068 -134.407760 -7.359082 -1.174759 6.160488 -1.174759 54.330124 6.160488 1.416667 0.409941 0.942128 1.328057 2.408976 3.143696 0.440581 -17.602177 -21.787668 10.649715 134.892646 In\u00a0[4]: Copied! <pre>num = 500\np = seqs[['bat_side', 'pitch_type', 'plate_x', 'plate_z']].head(num)\n\nfig, ax = plt.subplots()\n\nax.scatter(p['plate_x'], p['plate_z'], alpha=.5, edgecolors='black')\n\nbottom_zone = np.mean(seqs['strike_zone_bottom'])\ntop_zone = np.mean(seqs['strike_zone_top'])\nplate_width = 17/12\n\nog_zone = Rectangle((-plate_width / 2, bottom_zone), plate_width, top_zone - bottom_zone,\n                          edgecolor='black', fill=False, lw=3)\nax.add_patch(og_zone)\n\nexpansion_factor = 0.75\n\nexpanded_plate_width = plate_width * (1 + expansion_factor)\nexpanded_bottom_zone = bottom_zone - (top_zone - bottom_zone) * (expansion_factor / 2)\nexpanded_top_zone = top_zone + (top_zone - bottom_zone) * (expansion_factor / 2)\n\n# Draw the expanded rectangle\nexpanded_rect = Rectangle((-expanded_plate_width / 2, expanded_bottom_zone), expanded_plate_width,\n                          expanded_top_zone - expanded_bottom_zone, edgecolor='red', fill=False, lw=3)\nax.add_patch(expanded_rect)\n\nax.set_title(f'Sample of {num} Pitches Before Filtering')\n\nax.set_xlim(-expanded_plate_width, expanded_plate_width)\nax.set_ylim(expanded_bottom_zone - 1, expanded_top_zone + 1)\n\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n</pre> num = 500 p = seqs[['bat_side', 'pitch_type', 'plate_x', 'plate_z']].head(num)  fig, ax = plt.subplots()  ax.scatter(p['plate_x'], p['plate_z'], alpha=.5, edgecolors='black')  bottom_zone = np.mean(seqs['strike_zone_bottom']) top_zone = np.mean(seqs['strike_zone_top']) plate_width = 17/12  og_zone = Rectangle((-plate_width / 2, bottom_zone), plate_width, top_zone - bottom_zone,                           edgecolor='black', fill=False, lw=3) ax.add_patch(og_zone)  expansion_factor = 0.75  expanded_plate_width = plate_width * (1 + expansion_factor) expanded_bottom_zone = bottom_zone - (top_zone - bottom_zone) * (expansion_factor / 2) expanded_top_zone = top_zone + (top_zone - bottom_zone) * (expansion_factor / 2)  # Draw the expanded rectangle expanded_rect = Rectangle((-expanded_plate_width / 2, expanded_bottom_zone), expanded_plate_width,                           expanded_top_zone - expanded_bottom_zone, edgecolor='red', fill=False, lw=3) ax.add_patch(expanded_rect)  ax.set_title(f'Sample of {num} Pitches Before Filtering')  ax.set_xlim(-expanded_plate_width, expanded_plate_width) ax.set_ylim(expanded_bottom_zone - 1, expanded_top_zone + 1)  plt.gca().set_aspect('equal', adjustable='box') plt.show()  In\u00a0[5]: Copied! <pre>expanded_zone = -expanded_plate_width / 2, expanded_plate_width / 2, expanded_bottom_zone, expanded_top_zone\nx_min, x_max, z_min, z_max = expanded_zone\n</pre> expanded_zone = -expanded_plate_width / 2, expanded_plate_width / 2, expanded_bottom_zone, expanded_top_zone x_min, x_max, z_min, z_max = expanded_zone In\u00a0[6]: Copied! <pre>expanded_zone\n</pre> expanded_zone Out[6]: <pre>(-1.2395833333333335,\n 1.2395833333333335,\n 0.9316669711171024,\n 4.0241463664715384)</pre> In\u00a0[7]: Copied! <pre>def filter_wild():\n\n  orig_len = len(seqs)\n  expanded_zone = -expanded_plate_width / 2, expanded_plate_width / 2, expanded_bottom_zone, expanded_top_zone\n  x_min, x_max, z_min, z_max = expanded_zone\n\n  filtered = seqs[(seqs['plate_x'] &gt;= x_min) &amp; (seqs['plate_x'] &lt;= x_max) &amp;\n                 (seqs['plate_z'] &gt;= z_min) &amp; (seqs['plate_z'] &lt;= z_max)]\n\n  final_len = len(filtered)\n\n  print(f'Filtered out {abs(final_len - orig_len)} pitches')\n\n  return filtered\n</pre> def filter_wild():    orig_len = len(seqs)   expanded_zone = -expanded_plate_width / 2, expanded_plate_width / 2, expanded_bottom_zone, expanded_top_zone   x_min, x_max, z_min, z_max = expanded_zone    filtered = seqs[(seqs['plate_x'] &gt;= x_min) &amp; (seqs['plate_x'] &lt;= x_max) &amp;                  (seqs['plate_z'] &gt;= z_min) &amp; (seqs['plate_z'] &lt;= z_max)]    final_len = len(filtered)    print(f'Filtered out {abs(final_len - orig_len)} pitches')    return filtered In\u00a0[8]: Copied! <pre>seqs = filter_wild()\n</pre> seqs = filter_wild() <pre>Filtered out 158880 pitches\n</pre> In\u00a0[9]: Copied! <pre>num = 500\np = seqs[['bat_side', 'pitch_type', 'plate_x', 'plate_z']].head(num)\n\nfig, ax = plt.subplots()\n\nax.scatter(p['plate_x'], p['plate_z'], alpha=.5, edgecolors='black')\n\nbottom_zone = np.mean(seqs['strike_zone_bottom'])\ntop_zone = np.mean(seqs['strike_zone_top'])\nplate_width = 17/12\n\nog_zone = Rectangle((-plate_width / 2, bottom_zone), plate_width, top_zone - bottom_zone,\n                          edgecolor='black', fill=False, lw=3)\nax.add_patch(og_zone)\n\nexpansion_factor = 0.75\n\nexpanded_plate_width = plate_width * (1 + expansion_factor)\nexpanded_bottom_zone = bottom_zone - (top_zone - bottom_zone) * (expansion_factor / 2)\nexpanded_top_zone = top_zone + (top_zone - bottom_zone) * (expansion_factor / 2)\n\n# Draw the expanded rectangle\nexpanded_rect = Rectangle((-expanded_plate_width / 2, expanded_bottom_zone), expanded_plate_width,\n                          expanded_top_zone - expanded_bottom_zone, edgecolor='red', fill=False, lw=3)\nax.add_patch(expanded_rect)\n\n\nax.set_title(f'Sample of {num} Pitches After Filtering')\n\nax.set_xlim(-expanded_plate_width, expanded_plate_width)\nax.set_ylim(expanded_bottom_zone - 1, expanded_top_zone + 1)\n\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()\n</pre> num = 500 p = seqs[['bat_side', 'pitch_type', 'plate_x', 'plate_z']].head(num)  fig, ax = plt.subplots()  ax.scatter(p['plate_x'], p['plate_z'], alpha=.5, edgecolors='black')  bottom_zone = np.mean(seqs['strike_zone_bottom']) top_zone = np.mean(seqs['strike_zone_top']) plate_width = 17/12  og_zone = Rectangle((-plate_width / 2, bottom_zone), plate_width, top_zone - bottom_zone,                           edgecolor='black', fill=False, lw=3) ax.add_patch(og_zone)  expansion_factor = 0.75  expanded_plate_width = plate_width * (1 + expansion_factor) expanded_bottom_zone = bottom_zone - (top_zone - bottom_zone) * (expansion_factor / 2) expanded_top_zone = top_zone + (top_zone - bottom_zone) * (expansion_factor / 2)  # Draw the expanded rectangle expanded_rect = Rectangle((-expanded_plate_width / 2, expanded_bottom_zone), expanded_plate_width,                           expanded_top_zone - expanded_bottom_zone, edgecolor='red', fill=False, lw=3) ax.add_patch(expanded_rect)   ax.set_title(f'Sample of {num} Pitches After Filtering')  ax.set_xlim(-expanded_plate_width, expanded_plate_width) ax.set_ylim(expanded_bottom_zone - 1, expanded_top_zone + 1)  plt.gca().set_aspect('equal', adjustable='box') plt.show()  In\u00a0[10]: Copied! <pre>def prepare_data():\n    merged_dfs = []\n\n    # Iterate over the DataFrame row by row\n    for i in range(len(seqs) - 1):\n        if i % 50000 == 0:\n          print('Row:', i)\n\n        current_row =   seqs.iloc[i]\n        next_row = seqs.iloc[i + 1]\n\n        if current_row['sequence2_id'] == next_row['sequence1_id']:\n            # Create a DataFrame with the merged rows\n            merged_row = pd.concat([current_row.add_suffix('_1'), next_row.add_suffix('_2')], axis=0)\n\n            # Append the merged row DataFrame to the list\n            merged_dfs.append(merged_row)\n\n    # Concatenate the merged chunks into a single DataFrame\n    merged_df = pd.concat(merged_dfs, axis=1).T\n    merged_df = merged_df.set_index(['sequence1_id_2']).drop(columns=['sequence1_id_1', 'sequence2_id_1', 'sequence2_id_2',\n                                                                      'bat_side_2', 'pitch_hand_2'])\n\n    exclude_cols = ['bat_side_1', 'pitch_hand_1', 'pitch_type_1', 'pitch_type_2']\n    diff_cols = [col for col in merged_df.columns if col not in exclude_cols]\n\n    for col in diff_cols:\n        col_1 = col[:-2] + '_1'\n        col_2 = col[:-2] + '_2'\n        merged_df[col[:-2] + '_diff'] = merged_df[col_2] - merged_df[col_1]\n\n    # Drop original _1 and _2 columns\n    merged_df.drop(columns=[col for col in diff_cols] , inplace=True)\n    merged_df = merged_df.rename_axis('sequence_id')\n    merged_df = merged_df.rename(columns={'bat_side_1': 'bat_side', 'pitch_hand_1': 'pitch_hand'})\n\n    trash = {'KN', 'EP', 'SC', 'FO', 'CS'}\n    merged_df = merged_df[~(merged_df['pitch_type_1'].isin(trash) | merged_df['pitch_type_2'].isin(trash))]\n\n    merged_df = merged_df.drop(columns=['strike_zone_bottom_diff', 'strike_zone_top_diff',\n                                        'plate_y_diff','plate_z_gravity_diff'])\n\n    print('Done!')\n\n    return merged_df\n</pre> def prepare_data():     merged_dfs = []      # Iterate over the DataFrame row by row     for i in range(len(seqs) - 1):         if i % 50000 == 0:           print('Row:', i)          current_row =   seqs.iloc[i]         next_row = seqs.iloc[i + 1]          if current_row['sequence2_id'] == next_row['sequence1_id']:             # Create a DataFrame with the merged rows             merged_row = pd.concat([current_row.add_suffix('_1'), next_row.add_suffix('_2')], axis=0)              # Append the merged row DataFrame to the list             merged_dfs.append(merged_row)      # Concatenate the merged chunks into a single DataFrame     merged_df = pd.concat(merged_dfs, axis=1).T     merged_df = merged_df.set_index(['sequence1_id_2']).drop(columns=['sequence1_id_1', 'sequence2_id_1', 'sequence2_id_2',                                                                       'bat_side_2', 'pitch_hand_2'])      exclude_cols = ['bat_side_1', 'pitch_hand_1', 'pitch_type_1', 'pitch_type_2']     diff_cols = [col for col in merged_df.columns if col not in exclude_cols]      for col in diff_cols:         col_1 = col[:-2] + '_1'         col_2 = col[:-2] + '_2'         merged_df[col[:-2] + '_diff'] = merged_df[col_2] - merged_df[col_1]      # Drop original _1 and _2 columns     merged_df.drop(columns=[col for col in diff_cols] , inplace=True)     merged_df = merged_df.rename_axis('sequence_id')     merged_df = merged_df.rename(columns={'bat_side_1': 'bat_side', 'pitch_hand_1': 'pitch_hand'})      trash = {'KN', 'EP', 'SC', 'FO', 'CS'}     merged_df = merged_df[~(merged_df['pitch_type_1'].isin(trash) | merged_df['pitch_type_2'].isin(trash))]      merged_df = merged_df.drop(columns=['strike_zone_bottom_diff', 'strike_zone_top_diff',                                         'plate_y_diff','plate_z_gravity_diff'])      print('Done!')      return merged_df In\u00a0[11]: Copied! <pre>DATA = prepare_data()\n</pre> DATA = prepare_data() <pre>Row: 0\nRow: 50000\nRow: 100000\nRow: 150000\nRow: 200000\nRow: 250000\nRow: 300000\nRow: 350000\nRow: 400000\nRow: 450000\nRow: 500000\nRow: 550000\n</pre> <pre>&lt;ipython-input-10-6fec00a29211&gt;:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  merged_df[col[:-2] + '_diff'] = merged_df[col_2] - merged_df[col_1]\n&lt;ipython-input-10-6fec00a29211&gt;:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  merged_df[col[:-2] + '_diff'] = merged_df[col_2] - merged_df[col_1]\n&lt;ipython-input-10-6fec00a29211&gt;:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  merged_df[col[:-2] + '_diff'] = merged_df[col_2] - merged_df[col_1]\n&lt;ipython-input-10-6fec00a29211&gt;:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  merged_df[col[:-2] + '_diff'] = merged_df[col_2] - merged_df[col_1]\n&lt;ipython-input-10-6fec00a29211&gt;:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  merged_df[col[:-2] + '_diff'] = merged_df[col_2] - merged_df[col_1]\n&lt;ipython-input-10-6fec00a29211&gt;:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  merged_df[col[:-2] + '_diff'] = merged_df[col_2] - merged_df[col_1]\n</pre> <pre>Done!\n</pre> In\u00a0[12]: Copied! <pre>DATA = pd.get_dummies(DATA, columns=['pitch_hand',\t'bat_side', 'pitch_type_1', 'pitch_type_2']).astype(float)\n</pre> DATA = pd.get_dummies(DATA, columns=['pitch_hand',\t'bat_side', 'pitch_type_1', 'pitch_type_2']).astype(float) In\u00a0[13]: Copied! <pre>DATA.shape\n</pre> DATA.shape Out[13]: <pre>(296846, 56)</pre> In\u00a0[14]: Copied! <pre>target = 'pitch_value_diff'\n\nX = DATA.drop(columns=[target, 'linear_weight_diff'])\ny = DATA[target]\n\nx_train,x_test,y_train,y_test=train_test_split(X,y,train_size=0.75)\n\nmodel=Sequential()\n\nmodel.add(Dense(16, activation='tanh',\n                activity_regularizer='l2'))\n\nmodel.add(Dense(16, activation='tanh',\n                activity_regularizer='l2'))\n\nmodel.add(Dense(1,activation='linear'))\n\noptimizer = optimizers.SGD()\n\nmodel.compile(optimizer=optimizer, loss='huber')\n\nfitted_model = model.fit(x_train, y_train, batch_size=2500,epochs=10,\n            verbose=1, validation_data=(x_test, y_test))\n\nmodel.evaluate(x_test, y_test, verbose=0)\n</pre> target = 'pitch_value_diff'  X = DATA.drop(columns=[target, 'linear_weight_diff']) y = DATA[target]  x_train,x_test,y_train,y_test=train_test_split(X,y,train_size=0.75)  model=Sequential()  model.add(Dense(16, activation='tanh',                 activity_regularizer='l2'))  model.add(Dense(16, activation='tanh',                 activity_regularizer='l2'))  model.add(Dense(1,activation='linear'))  optimizer = optimizers.SGD()  model.compile(optimizer=optimizer, loss='huber')  fitted_model = model.fit(x_train, y_train, batch_size=2500,epochs=10,             verbose=1, validation_data=(x_test, y_test))  model.evaluate(x_test, y_test, verbose=0) <pre>Epoch 1/10\n90/90 [==============================] - 1s 7ms/step - loss: 0.2523 - val_loss: 0.2228\nEpoch 2/10\n90/90 [==============================] - 0s 4ms/step - loss: 0.2159 - val_loss: 0.2096\nEpoch 3/10\n90/90 [==============================] - 0s 4ms/step - loss: 0.2055 - val_loss: 0.2012\nEpoch 4/10\n90/90 [==============================] - 0s 4ms/step - loss: 0.1980 - val_loss: 0.1946\nEpoch 5/10\n90/90 [==============================] - 0s 4ms/step - loss: 0.1917 - val_loss: 0.1888\nEpoch 6/10\n90/90 [==============================] - 0s 4ms/step - loss: 0.1864 - val_loss: 0.1840\nEpoch 7/10\n90/90 [==============================] - 0s 4ms/step - loss: 0.1819 - val_loss: 0.1798\nEpoch 8/10\n90/90 [==============================] - 0s 4ms/step - loss: 0.1779 - val_loss: 0.1758\nEpoch 9/10\n90/90 [==============================] - 0s 4ms/step - loss: 0.1738 - val_loss: 0.1716\nEpoch 10/10\n90/90 [==============================] - 0s 4ms/step - loss: 0.1695 - val_loss: 0.1671\n</pre> Out[14]: <pre>0.16708585619926453</pre> In\u00a0[15]: Copied! <pre>predictions = model.predict(x_test)\n\npredictions_array = np.array(predictions)\ny_test_array = np.array(y_test)\n\nsns.kdeplot(predictions_array, color='yellow', label='Predictions')\nsns.kdeplot(y_test_array, color='darkblue', label='Actual')\nplt.xlabel('Values')\nplt.ylabel('Density')\nplt.title(f'Distribution of Predicted and Actual Values')\nplt.legend()\nplt.show()\n</pre> predictions = model.predict(x_test)  predictions_array = np.array(predictions) y_test_array = np.array(y_test)  sns.kdeplot(predictions_array, color='yellow', label='Predictions') sns.kdeplot(y_test_array, color='darkblue', label='Actual') plt.xlabel('Values') plt.ylabel('Density') plt.title(f'Distribution of Predicted and Actual Values') plt.legend() plt.show() <pre>2320/2320 [==============================] - 3s 1ms/step\n</pre> In\u00a0[49]: Copied! <pre>predictions_df = pd.DataFrame(predictions, columns=['Prediction'])\nactual_df = pd.DataFrame(y_test.to_numpy(), columns=['Actual'])\n\n# Create a DataFrame for additional features\nadditional_features_df = x_test.copy().reset_index(drop=True)  # Assuming x_test contains additional features\n\n# Merge all DataFrames together\ncombined_results = pd.concat([predictions_df, actual_df, additional_features_df], axis=1)\n</pre> predictions_df = pd.DataFrame(predictions, columns=['Prediction']) actual_df = pd.DataFrame(y_test.to_numpy(), columns=['Actual'])  # Create a DataFrame for additional features additional_features_df = x_test.copy().reset_index(drop=True)  # Assuming x_test contains additional features  # Merge all DataFrames together combined_results = pd.concat([predictions_df, actual_df, additional_features_df], axis=1) In\u00a0[55]: Copied! <pre>some_cols = ['Prediction', 'Actual', 'induced_vert_break_diff', 'pitch_type_1_FA',\n                  'pitch_type_1_FC', 'pitch_type_1_FF', 'pitch_type_1_ST',\n                  'pitch_type_2_FA','pitch_type_2_FC', 'pitch_type_2_FF', 'pitch_type_2_ST']\n\ncombined_results[:].sort_values('Prediction').head(10)\n</pre> some_cols = ['Prediction', 'Actual', 'induced_vert_break_diff', 'pitch_type_1_FA',                   'pitch_type_1_FC', 'pitch_type_1_FF', 'pitch_type_1_ST',                   'pitch_type_2_FA','pitch_type_2_FC', 'pitch_type_2_FF', 'pitch_type_2_ST']  combined_results[:].sort_values('Prediction').head(10) Out[55]: Prediction Actual ax_diff ay_diff az_diff vx0_diff vy0_diff vz0_diff x0_diff z0_diff extension_diff cy_diff t0_diff bx_diff by_diff bz_diff cx_diff cz_diff release_x_diff release_y_diff release_z_diff plate_time_diff plate_x_diff plate_z_diff plate_x_line_diff plate_z_line_diff horz_break_diff vert_break_diff induced_vert_break_diff release_speed_diff pitch_hand_L pitch_hand_R bat_side_L bat_side_R pitch_type_1_CH pitch_type_1_CU pitch_type_1_FA pitch_type_1_FC pitch_type_1_FF pitch_type_1_FS pitch_type_1_KC pitch_type_1_SI pitch_type_1_SL pitch_type_1_ST pitch_type_1_SV pitch_type_2_CH pitch_type_2_CU pitch_type_2_FA pitch_type_2_FC pitch_type_2_FF pitch_type_2_FS pitch_type_2_KC pitch_type_2_SI pitch_type_2_SL pitch_type_2_ST pitch_type_2_SV 9931 -0.747871 -0.013181 12.995058 7.528392 2.374228 -6.006707 12.219185 -0.217026 -0.155189 0.192832 -0.157924 0.157924 0.004294 -6.438770 11.879485 -0.169324 0.009139 0.199780 0.009139 0.157924 0.199780 0.057654 -1.020283 -0.348610 -2.611572 0.131765 19.095472 -5.764496 4.329795 -12.140038 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 32819 -0.710222 -0.013181 -11.742229 -0.552275 8.495110 1.916348 2.830893 2.075428 0.101870 0.131756 0.350833 -0.350833 -0.001830 2.332228 2.909970 1.766553 0.022138 0.061725 0.022138 -0.350833 0.061725 0.005923 0.031994 1.363443 0.909643 0.738737 -10.531797 7.496473 8.400916 -3.050227 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 42543 -0.662655 -0.013181 -13.043511 -2.024903 9.839750 2.798753 2.148461 1.759854 0.109885 0.149216 -0.464054 0.464054 0.003855 3.098246 2.084534 1.562450 0.061530 0.118368 0.061530 0.464054 0.118368 0.009221 0.230426 1.436404 1.225830 0.713012 -11.944844 8.680704 10.116218 -2.332079 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 59990 -0.659689 -0.131302 14.831889 -7.370188 0.074473 -3.857928 15.692757 2.557188 -0.256472 -0.010695 0.073281 -0.073281 0.005799 -4.603746 15.906450 2.706017 -0.082883 -0.142538 -0.082883 -0.073281 -0.142538 0.063706 -0.059306 0.476845 -2.017931 1.280047 23.503499 -9.638423 2.291542 -16.031832 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 70560 -0.624198 0.057251 11.791956 -0.437836 -3.159470 1.130267 8.373400 4.748111 -0.148693 0.388756 -0.238927 0.238927 0.004381 0.662285 8.278504 4.955951 -0.165482 0.229709 -0.165482 0.238927 0.229709 0.034162 1.287446 1.665803 -0.001934 2.297916 15.472554 -7.585355 -1.705528 -8.452833 1.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 42177 -0.609343 -0.013181 21.381447 4.229733 -2.655137 -7.418267 -4.779729 3.029998 0.742953 -0.743385 0.372079 -0.372079 -0.004298 -8.083660 -4.818625 3.012330 0.966238 -0.852657 0.966238 -0.372079 -0.852657 -0.018488 -0.447944 0.459171 -2.485799 0.520568 24.454266 -0.736770 -3.907395 5.251214 1.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 9164 -0.604288 0.073599 26.547827 -1.484652 -6.049841 -1.683949 9.123160 4.614918 -0.063269 0.066321 0.131886 -0.131886 0.001480 -2.573149 9.135718 4.862798 -0.000415 -0.091887 -0.000415 -0.131886 -0.091887 0.037088 1.869626 0.987607 -0.980507 2.165908 34.201602 -14.139611 -7.461732 -9.261815 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 59163 -0.602292 0.617323 2.448565 0.113721 2.839330 -0.718227 4.653558 2.519401 -0.164335 -0.647872 0.028200 -0.028200 0.001841 -0.860127 4.603220 2.428810 -0.133139 -0.777266 -0.133139 -0.028200 -0.777266 0.024102 -0.016178 0.606094 -0.477089 0.572681 5.530935 0.400958 5.215919 -4.544859 0.0 1.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 43932 -0.573687 0.020765 -20.377994 6.037756 -11.195154 1.102088 -10.487658 -2.409815 0.128648 0.483267 -0.273684 0.273684 -0.000346 1.712714 -10.659411 -2.082134 0.089818 0.550778 0.089818 0.273684 0.550778 -0.030743 -1.152847 -0.917504 0.477869 -0.311776 -19.568592 -7.268729 -12.243960 10.768075 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 41125 -0.568681 -0.199118 9.820816 -1.156005 2.437014 -1.728748 9.727321 1.629594 -0.270385 0.010061 0.129788 -0.129788 0.002744 -2.194258 9.721388 1.600430 -0.191683 -0.067963 -0.191683 -0.129788 -0.067963 0.042846 0.203484 0.472511 -1.099502 0.809501 15.635828 -4.043886 4.057840 -9.757303 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 In\u00a0[16]: Copied! <pre># Define a function to perform gradient ascent on input data\ndef gradient_ascent(input_data, model, learning_rate=0.1, num_iterations=100):\n    input_data = tf.convert_to_tensor(input_data)\n    for i in range(num_iterations):\n        with tf.GradientTape() as tape:\n            tape.watch(input_data)\n            # Compute the output of the model\n            output = model(input_data)\n        # Calculate gradients of the output with respect to the input data\n        gradients = tape.gradient(output, input_data)\n        # Update the input data using gradients to maximize activation\n        input_data = input_data + learning_rate * gradients\n    return input_data\n\n# Drop the target variable column from the DataFrame\ndata_without_target = DATA.drop(columns=['pitch_value_diff', 'linear_weight_diff'])\n\n# Convert the DataFrame to a NumPy array\ninput_data = data_without_target.values\n\n# Initialize input data (can be random or any initial input)\ninput_data = np.random.rand(1, 54)  # Assuming num_features is the number of input features\n\n# Perform gradient ascent to maximize the activation of the model\noptimized_input_data = gradient_ascent(input_data, model)\n\n# Display the optimized input data that maximally activates the model\nprint(\"Optimized input data:\", optimized_input_data)\n</pre> # Define a function to perform gradient ascent on input data def gradient_ascent(input_data, model, learning_rate=0.1, num_iterations=100):     input_data = tf.convert_to_tensor(input_data)     for i in range(num_iterations):         with tf.GradientTape() as tape:             tape.watch(input_data)             # Compute the output of the model             output = model(input_data)         # Calculate gradients of the output with respect to the input data         gradients = tape.gradient(output, input_data)         # Update the input data using gradients to maximize activation         input_data = input_data + learning_rate * gradients     return input_data  # Drop the target variable column from the DataFrame data_without_target = DATA.drop(columns=['pitch_value_diff', 'linear_weight_diff'])  # Convert the DataFrame to a NumPy array input_data = data_without_target.values  # Initialize input data (can be random or any initial input) input_data = np.random.rand(1, 54)  # Assuming num_features is the number of input features  # Perform gradient ascent to maximize the activation of the model optimized_input_data = gradient_ascent(input_data, model)  # Display the optimized input data that maximally activates the model print(\"Optimized input data:\", optimized_input_data)  <pre>Optimized input data: tf.Tensor(\n[[ 0.99747113  0.56107389  0.49479505 -0.42032312 -0.09616878  0.47514421\n   0.34939482  0.61791561  0.17803846 -0.30808163  0.20510365  0.88980291\n  -0.18395411  0.32808775  1.16395435  1.03163696  0.27685419  0.5248278\n   1.23585637  0.48856943  0.47256798  1.44639506  0.18730884  0.21241398\n  -0.08647076  0.4346417   0.53941275  0.14289344  0.31117563 -0.4231632\n   1.22124431 -0.47309395  0.29830113  0.69341215  0.83974674 -0.23372407\n   1.27338048  0.4912327   1.09447855  0.6541934   1.04859962  0.94869023\n   0.60814902  0.6710334   0.62781133  0.31563144  0.14640915 -0.32969948\n   0.93407113  1.07133669  0.39899053 -0.16179909  0.37407533  0.05894762]], shape=(1, 54), dtype=float64)\n</pre> In\u00a0[19]: Copied! <pre>optimized_input_data_np = optimized_input_data.numpy()\n\n# Flatten the numpy array\noptimized_input_data_flat = optimized_input_data_np.flatten()\n\n# Plot the values of the optimized input data\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(optimized_input_data_flat)), optimized_input_data_flat)\nplt.xlabel('Feature Index')\nplt.ylabel('Optimized Input Value')\nplt.title('Optimized Input Data for Activating Neuron')\nplt.show()\n</pre> optimized_input_data_np = optimized_input_data.numpy()  # Flatten the numpy array optimized_input_data_flat = optimized_input_data_np.flatten()  # Plot the values of the optimized input data plt.figure(figsize=(10, 6)) plt.bar(range(len(optimized_input_data_flat)), optimized_input_data_flat) plt.xlabel('Feature Index') plt.ylabel('Optimized Input Value') plt.title('Optimized Input Data for Activating Neuron') plt.show() In\u00a0[20]: Copied! <pre># Convert the optimized input data tensor to a NumPy array\noptimized_input_data_np = optimized_input_data.numpy()\n\n# Flatten the array\nflattened_data = optimized_input_data_np.flatten()\n\n# Retrieve the indices of the top 5 features with the highest optimized values\ntop5_indices = np.argsort(flattened_data)[-5:]\n\n# Print the indices of the top 5 features\nprint(\"Indices of the top 5 features:\", top5_indices)\n\n# Retrieve the corresponding feature names using the indices\ntop5_feature_names = DATA.columns[top5_indices]\n\n# Print the names of the top 5 features\nprint(\"Names of the top 5 features:\", top5_feature_names)\n</pre> # Convert the optimized input data tensor to a NumPy array optimized_input_data_np = optimized_input_data.numpy()  # Flatten the array flattened_data = optimized_input_data_np.flatten()  # Retrieve the indices of the top 5 features with the highest optimized values top5_indices = np.argsort(flattened_data)[-5:]  # Print the indices of the top 5 features print(\"Indices of the top 5 features:\", top5_indices)  # Retrieve the corresponding feature names using the indices top5_feature_names = DATA.columns[top5_indices]  # Print the names of the top 5 features print(\"Names of the top 5 features:\", top5_feature_names)  <pre>Indices of the top 5 features: [14 30 18 36 21]\nNames of the top 5 features: Index(['by_diff', 'pitch_hand_L', 'release_x_diff', 'pitch_type_1_FA',\n       'plate_time_diff'],\n      dtype='object')\n</pre> In\u00a0[59]: Copied! <pre># Permutation feature importance\n\nbaseline_error = mean_squared_error(y_test, model.predict(x_test))\n\nfeature_importances = {}\nfor i, col in enumerate(x_train.columns):\n    x_test_permuted = x_test.copy()\n    x_test_permuted[col] = np.random.permutation(x_test_permuted[col])\n    permuted_error = mean_squared_error(y_test, model.predict(x_test_permuted))\n    feature_importances[col] = baseline_error - permuted_error\n\n# Sort feature importances\nsorted_feature_importances = sorted(feature_importances.items(), key=lambda x: x[1], reverse=True)\nsorted_feature_importances\n</pre> # Permutation feature importance  baseline_error = mean_squared_error(y_test, model.predict(x_test))  feature_importances = {} for i, col in enumerate(x_train.columns):     x_test_permuted = x_test.copy()     x_test_permuted[col] = np.random.permutation(x_test_permuted[col])     permuted_error = mean_squared_error(y_test, model.predict(x_test_permuted))     feature_importances[col] = baseline_error - permuted_error  # Sort feature importances sorted_feature_importances = sorted(feature_importances.items(), key=lambda x: x[1], reverse=True) sorted_feature_importances <pre>2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n2320/2320 [==============================] - 3s 1ms/step\n</pre> Out[59]: <pre>[('bat_side_L', 0.0004700349207348298),\n ('bat_side_R', 0.00040101885540504145),\n ('z0_diff', 9.019235727203423e-05),\n ('pitch_type_2_FS', 5.81150711670142e-05),\n ('release_x_diff', 5.676104006885341e-05),\n ('release_z_diff', 5.39028114218687e-05),\n ('x0_diff', 4.5821280999017167e-05),\n ('cx_diff', 3.4173539158216526e-05),\n ('pitch_type_1_CU', 1.2812579088847253e-05),\n ('pitch_type_2_SV', 4.676218014323941e-06),\n ('release_y_diff', 3.909339549967861e-06),\n ('pitch_type_1_SV', 2.172830204649756e-06),\n ('pitch_type_2_FA', 8.494175655210379e-07),\n ('cz_diff', 8.208956867178374e-07),\n ('pitch_type_1_KC', 3.5998157449368406e-07),\n ('t0_diff', -2.4783319771404244e-08),\n ('plate_time_diff', -1.0059922315314163e-06),\n ('pitch_type_2_SL', -3.27418869690288e-06),\n ('pitch_type_2_SI', -4.545253858595499e-06),\n ('pitch_type_2_CU', -5.980296902255433e-06),\n ('pitch_type_1_FS', -7.385218282987549e-06),\n ('pitch_type_1_FC', -7.620443112754671e-06),\n ('pitch_type_1_FA', -8.17855728470207e-06),\n ('pitch_type_2_CH', -1.0435606461717262e-05),\n ('pitch_type_2_FC', -2.1457587275758305e-05),\n ('pitch_type_1_ST', -4.464337897346071e-05),\n ('pitch_type_2_KC', -6.662806806284616e-05),\n ('pitch_type_2_ST', -6.918082876278264e-05),\n ('extension_diff', -0.00011400639981205829),\n ('cy_diff', -0.00012220243812118003),\n ('pitch_type_1_CH', -0.00014538764234652002),\n ('pitch_type_1_SI', -0.00020464214527819713),\n ('pitch_type_1_SL', -0.0002528286768712362),\n ('pitch_type_1_FF', -0.0002559732331797182),\n ('pitch_hand_R', -0.0004601367466443712),\n ('pitch_hand_L', -0.0004745006309233163),\n ('pitch_type_2_FF', -0.0004918950812428935),\n ('plate_x_line_diff', -0.0007128824362132857),\n ('plate_x_diff', -0.0010599955177053727),\n ('bz_diff', -0.0014061194406291833),\n ('plate_z_line_diff', -0.0031620656076388892),\n ('vz0_diff', -0.003618280684177355),\n ('ay_diff', -0.004115437801017252),\n ('vx0_diff', -0.005144819347782784),\n ('plate_z_diff', -0.007737380727052656),\n ('bx_diff', -0.015019729843935306),\n ('az_diff', -0.01598665582884562),\n ('vert_break_diff', -0.025514341303699686),\n ('by_diff', -0.03499921429453204),\n ('vy0_diff', -0.06298648038790935),\n ('release_speed_diff', -0.0836203945203905),\n ('induced_vert_break_diff', -0.12900615280940797),\n ('horz_break_diff', -0.14398232609870795),\n ('ax_diff', -0.19324181523401465)]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"final_ml_project/pitch_nn/#more-data-prep","title":"More Data Prep\u00b6","text":""},{"location":"final_ml_project/pitch_nn/#remove-bad-pitches","title":"Remove Bad Pitches\u00b6","text":""},{"location":"final_ml_project/pitch_nn/#slice-dice","title":"Slice &amp; Dice\u00b6","text":""},{"location":"final_ml_project/pitch_nn/#run","title":"RUN\u00b6","text":""},{"location":"final_ml_project/pitch_nn/#check-her-out","title":"Check Her Out\u00b6","text":""},{"location":"final_ml_project/pitch_outcome_model_r/","title":"Pitch outcome model r","text":"<p>To day we are going to estimate a very simple pitch outcome model and calculate (a) the run value of each pitch and (b) the expected run value of each pitch based on its characteristics.</p> In\u00a0[2]: Copied! <pre>pitch &lt;- data.table::fread(\"mlb_pitch_2023.csv\") # data.table::fread is a LOT faster than read.csv#\nlinear_weight &lt;- read.csv(\"linear_weight.csv\")                                                    #\ncount_value &lt;- read.csv(\"count_value.csv\")                                                        #\n</pre> pitch &lt;- data.table::fread(\"mlb_pitch_2023.csv\") # data.table::fread is a LOT faster than read.csv# linear_weight &lt;- read.csv(\"linear_weight.csv\")                                                    # count_value &lt;- read.csv(\"count_value.csv\")                                                        # <p>Below, we provide three functions that will come in handy in this tutorial:</p> <ul> <li><code>get_quadratic_coef</code> calculates the nine quadratic coefficients describing the trajectory of the pitch and returns the input data frame with these coefficients appended</li> <li><code>get_trackman_metrics</code> calculates TrackMan metrics like release point, break, plate location, etc.</li> <li><code>estimate_population_variance</code> estimates population variance in true talent from noisy performance by many players</li> </ul> In\u00a0[3]: Copied! <pre>get_quadratic_coef &lt;- function(data) {                                                            #\n\n  quadratic_coef &lt;- data |&gt;                                                                       #\n    dplyr::mutate(                                                                                #\n      cy = 60.5 - extension,                                                                      #\n      # y0 is the value of y at the time when y = 50 (so it is defintionally 50)                  #\n      y0 = 50,                                                                                    #\n      # t0 is the time corresponding to vx0, vy0, vz0, x0, z0 (i.e. the time when y = 50)         #\n      # Calculate time from y0 to release point, and then negate that time                        #\n      t0 = -(-vy0 - sqrt(vy0^2 - 4 * (ay / 2) * (y0 - cy))) / (2 * (ay / 2)),                     #\n      # Backtrack velocities by t0 time                                                           #\n      bx = vx0 + (-t0) * ax,                                                                      #\n      by = vy0 + (-t0) * ay,                                                                      #\n      bz = vz0 + (-t0) * az,                                                                      #\n      # Backtrack locations by t0 time                                                            #\n      cx = x0 + (-t0) * vx0 + (-t0)^2 * ax / 2,                                                   #\n      cz = z0 + (-t0) * vz0 + (-t0)^2 * az / 2                                                    #\n    )                                                                                             #\n\n  return(quadratic_coef)                                                                          #\n}                                                                                                 #\n\nget_trackman_metrics &lt;- function(data) {                                                          #\n\n  trackman_metrics &lt;- data |&gt;                                                                     #\n    dplyr::mutate(                                                                                #\n\n      release_x = cx,                                                                             #\n      release_y = cy,                                                                             #\n      release_z = cz,                                                                             #\n\n      # Calculate plate location                                                                  #\n      plate_y = 17 / 12,  # back of home plate is zero; front is 17 inches                        #\n      # Solve quadratic equation to get the time at which ball reaches front of plate             #\n      plate_time = (-by - sqrt(by^2 - 4 * (ay / 2) * (cy - plate_y))) / (2 * (ay / 2)),           #\n      plate_x = ax * plate_time^2 / 2 + bx * plate_time + cx,                                     #\n      plate_z = az * plate_time^2 / 2 + bz * plate_time + cz,                                     #\n\n      # Set up some intermediate variables for calculating breaks                                 #\n      gravity = -32.17,   # feet per second per second                                            #\n      plate_x_line = bx * plate_time + cx,                                                        #\n      plate_z_line = bz * plate_time + cz,                                                        #\n      plate_z_gravity = gravity * plate_time^2 / 2 + bz * plate_time + cz,                        #\n\n      # SP: I'm reconstructing these from memory, so not 100% sure they're correct                #\n      horz_break = 12 * (plate_x - plate_x_line),               # measured in inches              #\n      vert_break = 12 * (plate_z - plate_z_line),               # measured in inches              #\n      induced_vert_break = 12 * (plate_z - plate_z_gravity),    # measured in inches              #\n\n      # Also recover the TrackMan metrics necessary for calculating the quadratic coefficients    #\n      # t0 is the time when y = 50 (necessary for calculating velocity and x/z location at time t0)\n      t0 = (-by - sqrt(by^2 - 4 * (ay / 2) * (cy - 50))) / (2 * (ay / 2)),                        #\n      vx0 = ax * t0 + bx,                                                                         #\n      vy0 = ay * t0 + by,                                                                         #\n      vz0 = az * t0 + bz,                                                                         #\n      x0 = ax * t0^2 / 2 + bx * t0 + cx,                                                          #\n      z0 = az * t0^2 / 2 + bz * t0 + cz,                                                          #\n      extension = 60.5 - cy,                                                                      #\n      release_speed = sqrt(bx^2 + by^2 + bz^2)                                                    #\n    )                                                                                             #\n\n  return(trackman_metrics)                                                                        #\n}                                                                                                 #\n\nestimate_population_variance &lt;- function(observed_value,                                          #\n                                         noise_variance,                                          #\n                                         population_mean = weighted.mean(observed_value, w = 1 / noise_variance, na.rm = TRUE),\n                                         max_iterations = 1e4,                                    #\n                                         tolerance = 1e-7 * var(observed_value)) {                #\n\n  point_estimate &lt;- (observed_value - population_mean)^2 - noise_variance                         #\n  last_population_variance &lt;- 0                                                                   #\n  population_variance &lt;- mean((observed_value - population_mean)^2)                               #\n\n  t &lt;- 0                                                                                          #\n  while(abs(population_variance - last_population_variance) &gt; tolerance &amp; t &lt; max_iterations) {   #\n\n    t &lt;- t + 1                                                                                    #\n    last_population_variance &lt;- population_variance                                               #\n    weight &lt;- (noise_variance + population_variance)^{-2}                                         #\n    population_variance &lt;- weighted.mean(point_estimate, w = weight)                              #\n  }                                                                                               #\n\n  return(population_variance)                                                                     #\n}                                                                                                 #\n</pre> get_quadratic_coef &lt;- function(data) {                                                            #    quadratic_coef &lt;- data |&gt;                                                                       #     dplyr::mutate(                                                                                #       cy = 60.5 - extension,                                                                      #       # y0 is the value of y at the time when y = 50 (so it is defintionally 50)                  #       y0 = 50,                                                                                    #       # t0 is the time corresponding to vx0, vy0, vz0, x0, z0 (i.e. the time when y = 50)         #       # Calculate time from y0 to release point, and then negate that time                        #       t0 = -(-vy0 - sqrt(vy0^2 - 4 * (ay / 2) * (y0 - cy))) / (2 * (ay / 2)),                     #       # Backtrack velocities by t0 time                                                           #       bx = vx0 + (-t0) * ax,                                                                      #       by = vy0 + (-t0) * ay,                                                                      #       bz = vz0 + (-t0) * az,                                                                      #       # Backtrack locations by t0 time                                                            #       cx = x0 + (-t0) * vx0 + (-t0)^2 * ax / 2,                                                   #       cz = z0 + (-t0) * vz0 + (-t0)^2 * az / 2                                                    #     )                                                                                             #    return(quadratic_coef)                                                                          # }                                                                                                 #  get_trackman_metrics &lt;- function(data) {                                                          #    trackman_metrics &lt;- data |&gt;                                                                     #     dplyr::mutate(                                                                                #        release_x = cx,                                                                             #       release_y = cy,                                                                             #       release_z = cz,                                                                             #        # Calculate plate location                                                                  #       plate_y = 17 / 12,  # back of home plate is zero; front is 17 inches                        #       # Solve quadratic equation to get the time at which ball reaches front of plate             #       plate_time = (-by - sqrt(by^2 - 4 * (ay / 2) * (cy - plate_y))) / (2 * (ay / 2)),           #       plate_x = ax * plate_time^2 / 2 + bx * plate_time + cx,                                     #       plate_z = az * plate_time^2 / 2 + bz * plate_time + cz,                                     #        # Set up some intermediate variables for calculating breaks                                 #       gravity = -32.17,   # feet per second per second                                            #       plate_x_line = bx * plate_time + cx,                                                        #       plate_z_line = bz * plate_time + cz,                                                        #       plate_z_gravity = gravity * plate_time^2 / 2 + bz * plate_time + cz,                        #        # SP: I'm reconstructing these from memory, so not 100% sure they're correct                #       horz_break = 12 * (plate_x - plate_x_line),               # measured in inches              #       vert_break = 12 * (plate_z - plate_z_line),               # measured in inches              #       induced_vert_break = 12 * (plate_z - plate_z_gravity),    # measured in inches              #        # Also recover the TrackMan metrics necessary for calculating the quadratic coefficients    #       # t0 is the time when y = 50 (necessary for calculating velocity and x/z location at time t0)       t0 = (-by - sqrt(by^2 - 4 * (ay / 2) * (cy - 50))) / (2 * (ay / 2)),                        #       vx0 = ax * t0 + bx,                                                                         #       vy0 = ay * t0 + by,                                                                         #       vz0 = az * t0 + bz,                                                                         #       x0 = ax * t0^2 / 2 + bx * t0 + cx,                                                          #       z0 = az * t0^2 / 2 + bz * t0 + cz,                                                          #       extension = 60.5 - cy,                                                                      #       release_speed = sqrt(bx^2 + by^2 + bz^2)                                                    #     )                                                                                             #    return(trackman_metrics)                                                                        # }                                                                                                 #  estimate_population_variance &lt;- function(observed_value,                                          #                                          noise_variance,                                          #                                          population_mean = weighted.mean(observed_value, w = 1 / noise_variance, na.rm = TRUE),                                          max_iterations = 1e4,                                    #                                          tolerance = 1e-7 * var(observed_value)) {                #    point_estimate &lt;- (observed_value - population_mean)^2 - noise_variance                         #   last_population_variance &lt;- 0                                                                   #   population_variance &lt;- mean((observed_value - population_mean)^2)                               #    t &lt;- 0                                                                                          #   while(abs(population_variance - last_population_variance) &gt; tolerance &amp; t &lt; max_iterations) {   #      t &lt;- t + 1                                                                                    #     last_population_variance &lt;- population_variance                                               #     weight &lt;- (noise_variance + population_variance)^{-2}                                         #     population_variance &lt;- weighted.mean(point_estimate, w = weight)                              #   }                                                                                               #    return(population_variance)                                                                     # }                                                                                                 # <p>Here we calculate deciles for each quadratic coefficient to be used later for our simple pitch outcome model.</p> In\u00a0[4]: Copied! <pre>pitch_pct &lt;- pitch |&gt;                                                                             #\n  get_quadratic_coef() |&gt;                                                                         #\n  get_trackman_metrics() |&gt;                                                                       #\n  dplyr::arrange(plate_x) |&gt;                                                                      #\n  dplyr::mutate(plate_x_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 5)) |&gt;                      #\n  dplyr::arrange(plate_z) |&gt;                                                                      #\n  dplyr::mutate(plate_z_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 5)) |&gt;                      #\n  dplyr::arrange(release_speed) |&gt;                                                                #\n  dplyr::mutate(release_speed_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 10)) |&gt;               #\n  dplyr::arrange(horz_break) |&gt;                                                                   #\n  dplyr::mutate(horz_break_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 10)) |&gt;                  #\n  dplyr::arrange(induced_vert_break) |&gt;                                                           #\n  dplyr::mutate(induced_vert_break_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 10))             #\n</pre> pitch_pct &lt;- pitch |&gt;                                                                             #   get_quadratic_coef() |&gt;                                                                         #   get_trackman_metrics() |&gt;                                                                       #   dplyr::arrange(plate_x) |&gt;                                                                      #   dplyr::mutate(plate_x_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 5)) |&gt;                      #   dplyr::arrange(plate_z) |&gt;                                                                      #   dplyr::mutate(plate_z_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 5)) |&gt;                      #   dplyr::arrange(release_speed) |&gt;                                                                #   dplyr::mutate(release_speed_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 10)) |&gt;               #   dplyr::arrange(horz_break) |&gt;                                                                   #   dplyr::mutate(horz_break_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 10)) |&gt;                  #   dplyr::arrange(induced_vert_break) |&gt;                                                           #   dplyr::mutate(induced_vert_break_pct = floor((1:dplyr::n() - 1) / dplyr::n() * 10))             # <p>EXERCISE #1</p> <p>Create a new dataframe <code>data</code> that starts with <code>pitch_pct</code> and appends the following logical columns to reflect the pitch outcome tree:</p> <ul> <li><code>is_swing</code></li> <li><code>is_take</code></li> <li><code>is_hbp</code></li> <li><code>is_call</code></li> <li><code>is_strike</code></li> <li><code>is_ball</code></li> <li><code>is_contact</code></li> <li><code>is_miss</code></li> <li><code>is_fair</code></li> <li><code>is_foul</code></li> </ul> <p>You wil need to manually interpret the <code>description</code> column to determine each logical.</p> In\u00a0[5]: Copied! <pre>data &lt;- pitch_pct |&gt;                                                                              #\n  dplyr::mutate(                                                                                  #\n    is_swing = description %in% c(                                                                #\n      \"Foul\", \"Foul Bunt\", \"Foul Tip\", \"In play, no out\", \"In play, out(s)\", \"In play, run(s)\",   #\n      \"Missed Bunt\", \"Swinging Strike\", \"Swinging Strike (Blocked)\"                               #\n    ),                                                                                            #\n    is_take = !is_swing,                                                                          #\n    is_hbp = description %in% c(\"Hit By Pitch\"),                                                  #\n    is_call = is_take &amp; !is_hbp,                                                                  #\n    is_strike = description %in% c(\"Called Strike\"),                                              #\n    is_ball = is_call &amp; !is_strike,                                                               #\n    is_contact = description %in% c(\"Foul\", \"In play, no out\", \"In play, out(s)\", \"In play, run(s)\"),\n    is_miss = is_swing &amp; !is_contact,                                                             #\n    is_fair = description %in% c(\"In play, no out\", \"In play, out(s)\", \"In play, run(s)\"),        #\n    is_foul = is_contact &amp; !is_fair                                                               #\n  )                                                                                               #\n</pre> data &lt;- pitch_pct |&gt;                                                                              #   dplyr::mutate(                                                                                  #     is_swing = description %in% c(                                                                #       \"Foul\", \"Foul Bunt\", \"Foul Tip\", \"In play, no out\", \"In play, out(s)\", \"In play, run(s)\",   #       \"Missed Bunt\", \"Swinging Strike\", \"Swinging Strike (Blocked)\"                               #     ),                                                                                            #     is_take = !is_swing,                                                                          #     is_hbp = description %in% c(\"Hit By Pitch\"),                                                  #     is_call = is_take &amp; !is_hbp,                                                                  #     is_strike = description %in% c(\"Called Strike\"),                                              #     is_ball = is_call &amp; !is_strike,                                                               #     is_contact = description %in% c(\"Foul\", \"In play, no out\", \"In play, out(s)\", \"In play, run(s)\"),     is_miss = is_swing &amp; !is_contact,                                                             #     is_fair = description %in% c(\"In play, no out\", \"In play, out(s)\", \"In play, run(s)\"),        #     is_foul = is_contact &amp; !is_fair                                                               #   )                                                                                               # <p>EXERCISE #2</p> <p>Calculate the run value of each pitch. If the pitch terminates a plate appearance, its run value is the difference between the linear weight of the plate appearance and the value of the count before pitch. If the pitch does not terminate a plate appearance, its run value is the difference between pre-pitch count value and post-pitch count value.</p> <p>Create a new dataframe <code>data_with_value</code> that starts with <code>data</code> and appends the column <code>pitch_value</code>.</p> In\u00a0[6]: Copied! <pre>data_with_value &lt;- data |&gt;                                                                        #\n  dplyr::left_join(linear_weight, by = \"event\") |&gt;                                                #\n  dplyr::mutate(                                                                                  #\n    balls_plus_one = balls + 1,                                                                   #\n    strikes_plus_one = strikes + 1                                                                #\n  ) |&gt;                                                                                            #\n  dplyr::left_join(count_value, by = c(\"balls_plus_one\" = \"balls\", \"strikes\")) |&gt;                 #\n  dplyr::rename(count_value_ball = count_value) |&gt;                                                #\n  dplyr::left_join(count_value, by = c(\"balls\", \"strikes_plus_one\" = \"strikes\")) |&gt;               #\n  dplyr::rename(count_value_strike = count_value) |&gt;                                              #\n  dplyr::left_join(count_value, by = c(\"balls\", \"strikes\")) |&gt;                                    #\n  dplyr::mutate(                                                                                  #\n    end_value = dplyr::case_when(                                                                 #\n      is_fair ~ linear_weight,                                  # ball in play                    #\n      is_hbp ~ linear_weight,                                   # hit by pitch                    #\n      (balls == 3) &amp; is_ball ~ linear_weight,                   # walk                            #\n      (strikes == 2) &amp; (is_strike | is_miss) ~ linear_weight,   # strikeout                       #\n      (strikes == 2) &amp; is_foul ~ count_value,                   # two-strike foul                 #\n      is_ball ~ count_value_ball,                               # non-terminal ball               #\n      is_strike | is_miss | is_foul ~ count_value_strike        # non-terminal strike             #\n    ),                                                                                            #\n    pitch_value = end_value - count_value                                                         #\n  )                                                                                               #\n</pre> data_with_value &lt;- data |&gt;                                                                        #   dplyr::left_join(linear_weight, by = \"event\") |&gt;                                                #   dplyr::mutate(                                                                                  #     balls_plus_one = balls + 1,                                                                   #     strikes_plus_one = strikes + 1                                                                #   ) |&gt;                                                                                            #   dplyr::left_join(count_value, by = c(\"balls_plus_one\" = \"balls\", \"strikes\")) |&gt;                 #   dplyr::rename(count_value_ball = count_value) |&gt;                                                #   dplyr::left_join(count_value, by = c(\"balls\", \"strikes_plus_one\" = \"strikes\")) |&gt;               #   dplyr::rename(count_value_strike = count_value) |&gt;                                              #   dplyr::left_join(count_value, by = c(\"balls\", \"strikes\")) |&gt;                                    #   dplyr::mutate(                                                                                  #     end_value = dplyr::case_when(                                                                 #       is_fair ~ linear_weight,                                  # ball in play                    #       is_hbp ~ linear_weight,                                   # hit by pitch                    #       (balls == 3) &amp; is_ball ~ linear_weight,                   # walk                            #       (strikes == 2) &amp; (is_strike | is_miss) ~ linear_weight,   # strikeout                       #       (strikes == 2) &amp; is_foul ~ count_value,                   # two-strike foul                 #       is_ball ~ count_value_ball,                               # non-terminal ball               #       is_strike | is_miss | is_foul ~ count_value_strike        # non-terminal strike             #     ),                                                                                            #     pitch_value = end_value - count_value                                                         #   )                                                                                               # In\u00a0[10]: Copied! <pre>head(data_with_value)\n</pre> head(data_with_value) A data.table: 6 \u00d7 72 play_idgame_idevent_indexbatter_idbat_sidepitcher_idpitch_handeventplay_indexpitch_number\u22efis_fairis_foullinear_weightballs_plus_onestrikes_plus_onecount_value_ballcount_value_strikecount_valueend_valuepitch_value &lt;chr&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;chr&gt;&lt;int&gt;&lt;chr&gt;&lt;chr&gt;&lt;int&gt;&lt;int&gt;\u22ef&lt;lgl&gt;&lt;lgl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; a28c6983-e3a7-4956-894b-a928efc2570771771858516782R669467RWalk     12\u22efFALSEFALSE 0.332574621 0.09244198-0.01469748 0.0356157085 0.09244198 0.05682627 7a056759-e39e-4517-91fa-0f41d4da34e071804070683011R543056LStrikeout01\u22efFALSEFALSE-0.273826511 0.03561571-0.04188323 0.0000993257-0.04188323-0.04198255 9cbce12f-85ca-4ea7-99b1-9068d3bcabb971804069665828R543056LStrikeout54\u22efFALSEFALSE-0.273826523-0.03941095         NA-0.0786110430-0.03941095 0.03920009 eee8b4ea-fbac-467b-8bf4-83191a6e445771636715665839L543056LSingle   44\u22ef TRUEFALSE 0.474798123-0.03941095         NA-0.0786110430 0.47479812 0.55340916 b8ace30b-3989-4496-9e44-7ce12d9cc70571751558641505R669467RStrikeout34\u22efFALSEFALSE-0.273826523-0.03941095         NA-0.0786110430-0.27382649-0.19521545 a8a204d2-87dc-45ab-a710-cefdc5e7a1ed71812874592273R543056LStrikeout34\u22efFALSEFALSE-0.273826523-0.03941095         NA-0.0786110430-0.03941095 0.03920009 <p>EXERCISE #3</p> <p>For each combination of <code>bat_side</code> and each possible decile of the quadratic coefficients, calculate:</p> <ul> <li><code>prob_swing</code>: the probability of swing</li> <li><code>prob_hbp</code>: the probability of hbp given take</li> <li><code>prob_strike</code>: the probability of strike given call</li> <li><code>prob_contact</code>: the probability of contact given swing</li> <li><code>prob_fair</code>: the probability of fair given contact</li> <li><code>exp_outcome</code>: the expected outcome linear weight given fair</li> </ul> <p>Create a new dataframe <code>data_with_prob</code> that starts with <code>data_with_value</code> and appends the above columns for each pitch.</p> In\u00a0[7]: Copied! <pre>data_with_prob &lt;- data_with_value |&gt;                                                              #\n  dplyr::group_by(                                                                                #\n    bat_side, balls, strikes, plate_x_pct, plate_z_pct, release_speed_pct, horz_break_pct, induced_vert_break_pct #\n  ) |&gt;                                                                                            #\n  dplyr::mutate(                                                                                  #\n    prob_swing = mean(is_swing),                                                                  #\n    # Below, coalesce with zero to handle cases of no data in the relevant node of the outcome tree\n    prob_hbp = dplyr::coalesce(weighted.mean(is_hbp, w = is_take), 0),                            #\n    prob_strike = dplyr::coalesce(weighted.mean(is_strike, w = is_call), 0),                      #\n    prob_contact = dplyr::coalesce(weighted.mean(is_contact, w = is_swing), 0),                   #\n    prob_fair = dplyr::coalesce(weighted.mean(is_fair, w = is_contact), 0),                       #\n    exp_outcome = dplyr::coalesce(weighted.mean(linear_weight, w = is_fair), 0),                  #\n  ) |&gt;                                                                                            #\n  dplyr::ungroup()                                                                                #\n</pre> data_with_prob &lt;- data_with_value |&gt;                                                              #   dplyr::group_by(                                                                                #     bat_side, balls, strikes, plate_x_pct, plate_z_pct, release_speed_pct, horz_break_pct, induced_vert_break_pct #   ) |&gt;                                                                                            #   dplyr::mutate(                                                                                  #     prob_swing = mean(is_swing),                                                                  #     # Below, coalesce with zero to handle cases of no data in the relevant node of the outcome tree     prob_hbp = dplyr::coalesce(weighted.mean(is_hbp, w = is_take), 0),                            #     prob_strike = dplyr::coalesce(weighted.mean(is_strike, w = is_call), 0),                      #     prob_contact = dplyr::coalesce(weighted.mean(is_contact, w = is_swing), 0),                   #     prob_fair = dplyr::coalesce(weighted.mean(is_fair, w = is_contact), 0),                       #     exp_outcome = dplyr::coalesce(weighted.mean(linear_weight, w = is_fair), 0),                  #   ) |&gt;                                                                                            #   dplyr::ungroup()                                                                                # In\u00a0[13]: Copied! <pre>head(data_with_prob)\n</pre> head(data_with_prob) A tibble: 6 \u00d7 78 play_idgame_idevent_indexbatter_idbat_sidepitcher_idpitch_handeventplay_indexpitch_number\u22efcount_value_strikecount_valueend_valuepitch_valueprob_swingprob_hbpprob_strikeprob_contactprob_fairexp_outcome &lt;chr&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;chr&gt;&lt;int&gt;&lt;chr&gt;&lt;chr&gt;&lt;int&gt;&lt;int&gt;\u22ef&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; a28c6983-e3a7-4956-894b-a928efc2570771771858516782R669467RWalk     12\u22ef-0.01469748 0.0356157085 0.09244198 0.056826270.333333300.00000001.00000001.0-0.26425016 7a056759-e39e-4517-91fa-0f41d4da34e071804070683011R543056LStrikeout01\u22ef-0.04188323 0.0000993257-0.04188323-0.041982550.352941201.00000000.83333330.9 0.12166210 9cbce12f-85ca-4ea7-99b1-9068d3bcabb971804069665828R543056LStrikeout54\u22ef         NA-0.0786110430-0.03941095 0.039200090.400000000.16666671.00000000.5-0.27361691 eee8b4ea-fbac-467b-8bf4-83191a6e445771636715665839L543056LSingle   44\u22ef         NA-0.0786110430 0.47479812 0.553409161.000000000.00000001.00000000.5 0.09590723 b8ace30b-3989-4496-9e44-7ce12d9cc70571751558641505R669467RStrikeout34\u22ef         NA-0.0786110430-0.27382649-0.195215451.000000000.00000000.66666670.5 0.47479812 a8a204d2-87dc-45ab-a710-cefdc5e7a1ed71812874592273R543056LStrikeout34\u22ef         NA-0.0786110430-0.03941095 0.039200090.285714300.00000001.00000000.5-0.28298366 <p>EXERCISE #4</p> <p>Similar to Exercise #2, calculate the expected run value of each pitch by replacing the actual outcomes with the expected outcomes from Exercise #3.</p> In\u00a0[7]: Copied! <pre># We use the linear weight of a walk to calculate run value for balls in 3-ball counts            #\nlw_walk &lt;- linear_weight |&gt;                                                                       #\n  dplyr::filter(event == \"Walk\") |&gt;                                                               #\n  with(linear_weight)                                                                             #\n\n# We use the linear weight of a strike to calculate run value for strikes in 3-strike counts      #\nlw_strikeout &lt;- linear_weight |&gt;                                                                  #\n  dplyr::filter(event == \"Strikeout\") |&gt;                                                          #\n  with(linear_weight)                                                                             #\n\n# We use the linear weight of a hit by pitch to calculate run value for HBPs                      #\nlw_hbp &lt;- linear_weight |&gt;                                                                        #\n  dplyr::filter(event == \"Hit By Pitch\") |&gt;                                                       #\n  with(linear_weight)                                                                             #\n\ndata_with_exp_value &lt;- data_with_prob |&gt;                                                          #\n  dplyr::mutate(                                                                                  #\n    value_ball = ifelse(balls == 3, lw_walk, count_value_ball),            # count vlaue after ball\n    value_strike = ifelse(strikes == 2, lw_strikeout, count_value_strike), # count value after strike\n    value_foul = ifelse(strikes == 2, count_value, count_value_strike),    # count value after foul\n    exp_value = -count_value +  # all outcomes are measured relative to count value               #\n      (1 - prob_swing) * (1 - prob_hbp) * (1 - prob_strike) * value_ball + # called ball          #\n      (1 - prob_swing) * (1 - prob_hbp) * prob_strike * value_strike +     # called strike        #\n      (1 - prob_swing) * prob_hbp * lw_hbp +                               # hit by pitcher       #\n      prob_swing * (1 - prob_contact) * value_strike +                     # swinging strike      #\n      prob_swing * prob_contact * (1 - prob_fair) * value_foul +           # foul ball            #\n      prob_swing * prob_contact * prob_fair * exp_outcome,                 # fair ball            #\n    residual_value = pitch_value - exp_value                                                      #\n  )                                                                                               #\n</pre> # We use the linear weight of a walk to calculate run value for balls in 3-ball counts            # lw_walk &lt;- linear_weight |&gt;                                                                       #   dplyr::filter(event == \"Walk\") |&gt;                                                               #   with(linear_weight)                                                                             #  # We use the linear weight of a strike to calculate run value for strikes in 3-strike counts      # lw_strikeout &lt;- linear_weight |&gt;                                                                  #   dplyr::filter(event == \"Strikeout\") |&gt;                                                          #   with(linear_weight)                                                                             #  # We use the linear weight of a hit by pitch to calculate run value for HBPs                      # lw_hbp &lt;- linear_weight |&gt;                                                                        #   dplyr::filter(event == \"Hit By Pitch\") |&gt;                                                       #   with(linear_weight)                                                                             #  data_with_exp_value &lt;- data_with_prob |&gt;                                                          #   dplyr::mutate(                                                                                  #     value_ball = ifelse(balls == 3, lw_walk, count_value_ball),            # count vlaue after ball     value_strike = ifelse(strikes == 2, lw_strikeout, count_value_strike), # count value after strike     value_foul = ifelse(strikes == 2, count_value, count_value_strike),    # count value after foul     exp_value = -count_value +  # all outcomes are measured relative to count value               #       (1 - prob_swing) * (1 - prob_hbp) * (1 - prob_strike) * value_ball + # called ball          #       (1 - prob_swing) * (1 - prob_hbp) * prob_strike * value_strike +     # called strike        #       (1 - prob_swing) * prob_hbp * lw_hbp +                               # hit by pitcher       #       prob_swing * (1 - prob_contact) * value_strike +                     # swinging strike      #       prob_swing * prob_contact * (1 - prob_fair) * value_foul +           # foul ball            #       prob_swing * prob_contact * prob_fair * exp_outcome,                 # fair ball            #     residual_value = pitch_value - exp_value                                                      #   )                                                                                               # In\u00a0[8]: Copied! <pre>data_with_exp_value\n</pre> data_with_exp_value A tibble: 714402 \u00d7 83 play_idgame_idevent_indexbatter_idbat_sidepitcher_idpitch_handeventplay_indexpitch_number\u22efprob_hbpprob_strikeprob_contactprob_fairexp_outcomevalue_ballvalue_strikevalue_foulexp_valueresidual_value &lt;chr&gt;&lt;int&gt;&lt;int&gt;&lt;int&gt;&lt;chr&gt;&lt;int&gt;&lt;chr&gt;&lt;chr&gt;&lt;int&gt;&lt;int&gt;\u22ef&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;dbl&gt; a28c6983-e3a7-4956-894b-a928efc2570771771858516782R669467RWalk        12\u22ef0.00.000000001.00000001.0000000-0.26425016 0.09244198-0.01469748-0.01469748-0.062071107 0.118897379 7a056759-e39e-4517-91fa-0f41d4da34e071804070683011R543056LStrikeout   01\u22ef0.01.000000000.83333330.9000000 0.12166210 0.03561571-0.04188323-0.04188323 0.001308858-0.043291411 9cbce12f-85ca-4ea7-99b1-9068d3bcabb971804069665828R543056LStrikeout   54\u22ef0.00.166666671.00000000.5000000-0.27361691-0.03941095-0.27382649-0.07861104-0.038922671 0.078122765 eee8b4ea-fbac-467b-8bf4-83191a6e445771636715665839L543056LSingle      44\u22ef0.00.000000001.00000000.5000000 0.09590723-0.03941095-0.27382649-0.07861104 0.087259135 0.466150025 b8ace30b-3989-4496-9e44-7ce12d9cc70571751558641505R669467RStrikeout   34\u22ef0.00.000000000.66666670.5000000 0.47479812-0.03941095-0.27382649-0.07861104 0.119397904-0.314613352 a8a204d2-87dc-45ab-a710-cefdc5e7a1ed71812874592273R543056LStrikeout   34\u22ef0.00.000000001.00000000.5000000-0.28298366-0.03941095-0.27382649-0.07861104-0.001196021 0.040396116 074332cd-135b-42d0-8c02-9467510db9be71771857607043L669467RGroundout   23\u22ef0.00.000000000.00000000.0000000 0.00000000 0.20545206 0.03316602 0.03316602-0.059275961 0.000000000 c2fe5c05-9a63-4676-9a89-c4ad199a948a71771335682626R669461LFlyout      12\u22ef0.01.000000001.00000000.5000000-0.26425016 0.09244198-0.01469748-0.01469748-0.100223721 0.049910536 b802bba3-5587-42d4-abd8-66cebc11316671812874592273R543056LStrikeout   12\u22ef0.01.000000001.00000000.6666667-0.09170433-0.01469748-0.09704659-0.09704659-0.053026461-0.002136904 fa58855b-488b-402c-ae13-64f75d975f4471771337500871R669461LWalk        33\u22ef0.00.142857140.00000000.0000000 0.00000000 0.03316602-0.07861104-0.07861104 0.031895344 0.015968152 23710b20-2fca-4ea9-9f33-1f03c7cf2b3071655343666139L543056LGroundout   34\u22ef0.00.000000000.00000000.0000000 0.00000000-0.03941095-0.27382649-0.07861104 0.039200094 0.000000000 0607ba4a-8720-4b94-8a5e-adec301d37ad71820766670541L669424RHome Run    31\u22ef0.00.860000000.88235290.5777778 0.09404792 0.03561571-0.04188323-0.04188323-0.001619165-0.040363389 3d79b465-8603-4208-b457-06431108af1f71761040671277L607625RSingle      12\u22ef0.00.000000000.75000000.3333333-0.28298366 0.09244198-0.01469748-0.01469748-0.059314397 0.009001212 de7e07fa-95f6-4d64-932d-3d34d51b117871824058621020R669467RGroundout   56\u22ef0.00.000000001.00000001.0000000-0.28298366 0.33257455-0.27382649 0.05218967 0.075198814-0.410372144 f8599e4a-03b3-4d6c-8ac2-6c4ef32ea2ac71865059608385L669467RSingle      45\u22ef0.00.000000000.70833330.4117647 0.29062417-0.03941095-0.27382649-0.07861104 0.045964390 0.507444770 fd21b2b6-d279-406f-8fc4-30f1b63fdb8171865061642133L669467RDouble      33\u22ef0.00.222222221.00000000.4000000 0.24857842 0.03316602-0.07861104-0.07861104 0.038716317 0.009147179 7b108ee6-5948-42b6-a585-d338ac82ddb871824057663611R669467RHit By Pitch55\u22ef0.50.000000000.00000000.0000000 0.00000000 0.05218967-0.27382649-0.03941095 0.243167238 0.151566623 b7a25e4c-ebbb-4c5d-b2ba-c9fcca07955671861875665923R543056LStrikeout   43\u22ef0.00.000000000.50000001.0000000-0.26425016-0.07861104-0.27382649-0.09704659-0.019649906 0.038085456 cb66c54a-f7f0-4daa-991f-f68aa0532ae871858360623993L669424RStrikeout   22\u22ef0.00.666666671.00000000.0000000 0.00000000 0.09244198-0.01469748-0.01469748-0.028885294-0.021427891 0b64eee4-f053-4f0f-ab74-78f1986b0bef71850748645277L607259RFlyout      23\u22ef0.00.000000000.00000000.0000000 0.00000000 0.03316602-0.07861104-0.07861104 0.047863496 0.000000000 dd11c4ad-3060-418b-9b0d-8ebfd321fcbe71747820660271L607625RGroundout   33\u22ef0.00.166666671.00000000.6000000-0.03038974-0.07861104-0.27382649-0.09704659 0.010488039-0.196425108 b8355078-f049-400b-85f2-2cfdb17607f471842047643376R669424RStrikeout   12\u22ef0.00.750000000.87500000.7142857-0.12393390-0.01469748-0.09704659-0.09704659-0.059503985 0.004340620 39ee33c1-1d57-4a64-b75a-1e871950b60871761080665742L502179RSingle      01\u22ef0.00.556818181.00000000.6666667-0.28298366 0.03561571-0.04188323-0.04188323-0.014067635 0.049584017 0fcb5e73-97f3-446f-b442-cd9bbb657dfa71771334502054R669461LFlyout      34\u22ef0.00.166666671.00000000.6000000-0.03038974-0.03941095-0.27382649-0.07861104 0.013222631 0.025977463 899c686d-292d-4d2e-8f1a-993d1f15fa3771771858516782R669467RWalk        01\u22ef0.00.052631580.25000000.0000000 0.00000000 0.03561571-0.04188323-0.04188323 0.018668788 0.016847595 23e54962-c825-4385-95a0-30962feb315b71747834623205L607625RSingle      45\u22ef0.00.666666670.74285710.5769231 0.01122697 0.05218967-0.27382649-0.03941095-0.045457935 0.559667000 4d997f13-7e6c-441c-a9dd-c8f85d65019771856034592885L607625RStrikeout   22\u22ef0.00.108108110.00000000.0000000 0.00000000 0.09244198-0.01469748-0.01469748 0.045243628 0.011582644 bcd953d3-9ad8-478d-b419-1845f3968f1a71771339607043L669461LStrikeout   23\u22ef0.01.000000000.00000000.0000000 0.00000000-0.07861104-0.27382649-0.09704659-0.176779898 0.000000000 de846802-370a-4b1a-bba5-bfc152f83b7471804071643396R543056LGroundout   01\u22ef0.00.272727271.00000000.0000000 0.00000000 0.03561571-0.04188323-0.04188323 0.009683404-0.051665958 11d046a2-13b4-4b74-bf0d-1aa23bae207371771335682626R669461LFlyout      01\u22ef0.00.067796610.00000000.0000000 0.00000000 0.03561571-0.04188323-0.04188323 0.030262218 0.005254165 \u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22f1\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee c607a33c-bf32-44ef-86b5-e01b0ba0911871803236668227R670950RFlyout          34\u22ef01.000000001.00000001.0000000-0.26425016 0.33257455 0.13828085 0.13828085-0.335525215-0.134177003 a2cdd50b-4877-4192-8d74-eba96e7bc83571689522457759R434378RStrikeout       01\u22ef00.900000001.00000000.6666667 0.09590723 0.03561571-0.04188323-0.04188323-0.023261737-0.018720817 2a7b69c8-f879-4666-9055-f8db607d890671827931500743R605483LStrikeout       12\u22ef00.000000000.00000000.0000000 0.00000000-0.01469748-0.09704659-0.09704659 0.027185751 0.000000000 431e345b-b7eb-4a33-bebf-89c8bd40415a718082 0680757L434378RFlyout          42\u22ef00.000000000.75000000.0000000 0.00000000-0.01469748-0.09704659-0.09704659 0.007809489 0.019376263 fca7550b-bf6f-4e09-9c70-cb8f92c7311a71808238642708R434378RStrikeout       01\u22ef00.916666670.66666670.2500000-0.26425016 0.03561571-0.04188323-0.04188323-0.044228189 0.002245635 b9be773c-4897-4bdd-8e97-abc4f6a7128871810284656305R642585RStrikeout       54\u22ef00.000000000.73333330.2727273-0.26425016-0.03941095-0.27382649-0.07861104-0.089185275-0.106030173 2d55bf1c-d784-4b06-9900-a10d99f2350e71689521646240L434378RStrikeout       01\u22ef00.090909090.00000000.0000000 0.00000000 0.03561571-0.04188323-0.04188323 0.028471025 0.007045358 1ba0ecc8-5c0c-414b-99ba-4be636a8150571807940665742L601713RReached on Error23\u22ef00.000000000.00000000.0000000 0.00000000-0.07861104-0.27382649-0.09704659 0.018435550 0.000000000 8b62adeb-e967-4546-a9e5-733e837cd47071791076607043L657024RStrikeout       12\u22ef01.000000000.75000000.6666667-0.27361691-0.01469748-0.09704659-0.09704659-0.114020137 0.058856772 dd0a3d3c-a8f8-41e3-a9c4-65b0863952db718082 0680757L434378RFlyout          53\u22ef00.333333330.77777780.2857143 0.10527398 0.03316602-0.07861104-0.07861104-0.014636552-0.049277014 b9c7b59b-a1e0-4484-92b1-a72950d3cdb5718082 0680757L434378RFlyout          31\u22ef00.318181820.58823530.1000000 0.47479812 0.03561571-0.04188323-0.04188323 0.001072969-0.043055523 0a193cc7-486f-4c7a-8c16-fcb2cb1480ab71744839669477R596295LSingle          12\u22ef00.000000000.00000000.0000000 0.00000000 0.09244198-0.01469748-0.01469748 0.056826272 0.000000000 b495adbb-5666-48d6-9f78-bfa1595b750f71877819453568L605483LDouble          23\u22ef00.000000000.00000000.0000000 0.00000000 0.03316602-0.07861104-0.07861104 0.047863496 0.000000000 12111872-f1d1-41f0-9f18-4a29fc7f04c171720320600869L434378RWalk            33\u22ef01.000000000.75000000.0000000 0.00000000 0.03316602-0.07861104-0.07861104-0.063913566 0.000000000 11238ee1-e48d-4253-b2de-ae7a622cc8ad71654684669394R670032RFlyout          12\u22ef00.000000000.00000000.0000000 0.00000000 0.09244198-0.01469748-0.01469748 0.056826272 0.000000000 7070c720-2525-45cc-b318-af8c40773ba371810285643376R642585RStrikeout       01\u22ef01.000000000.65000000.4615385 0.07763556 0.03561571-0.04188323-0.04188323-0.025684537-0.016298017 e8f9aaa3-b385-4ed7-91fb-75e6240f543671721463514888R650907RHome Run        01\u22ef00.000000000.00000000.0000000 0.00000000 0.03561571-0.04188323-0.04188323 0.035516383 0.000000000 9a284060-163e-47d2-974f-7552018818b871810275595281L642585RSingle          32\u22ef00.750000000.33333331.0000000 0.47479812-0.01469748-0.09704659-0.09704659 0.038292896 0.478388449 8c363ae6-05d3-460c-953d-cb62bc37bb71717203 0682928L434378RSingle          31\u22ef00.500000000.50000000.5000000-0.28298366 0.03561571-0.04188323-0.04188323-0.031525822-0.010456732 9a90cf8c-b1dd-46db-a52d-5c223215f8f4718427 7621043R641482LFlyout          23\u22ef00.000000001.00000000.0000000 0.00000000 0.03316602-0.07861104-0.07861104-0.063913566 0.000000000 bdd773f6-d55e-4626-869b-26d0203b1ef4716390 8608701R607644LFlyout          23\u22ef01.000000001.00000001.0000000-0.26425016 0.20545206 0.03316602 0.03316602-0.158414686-0.198277450 06cab86f-251f-4d42-82d4-500db8a0859d71833232669127R682243RStrikeout       44\u22ef00.000000000.66666670.2500000-0.28298366-0.03941095-0.27382649-0.07861104-0.099133919 0.099133919 76267209-840e-403d-b6f4-9b0a8fb3910371813564643446L621363LStrikeout       33\u22ef00.000000000.00000000.0000000 0.00000000 0.20545206 0.03316602 0.03316602 0.113010081 0.000000000 48dc6d19-a682-48d2-b41f-b1cfd840cec471868412571657R664353RStrikeout       34\u22ef01.000000001.00000001.0000000 0.57033441 0.13828085-0.03941095-0.03941095 0.293270249-0.365847216 a1adf2e9-de88-4a22-8c6c-f7a99510b68771842723456781R641482LDouble          01\u22ef00.000000000.00000000.0000000 0.00000000 0.03561571-0.04188323-0.04188323 0.035516383 0.000000000 a264ebda-1dc5-4088-8804-1e62ef922d4b71768246667452R434378RSingle          01\u22ef00.000000000.00000000.0000000 0.00000000 0.03561571-0.04188323-0.04188323 0.035516383 0.000000000 46356393-779c-4661-86a0-a46c61636bbb718099 4607043L676440RFlyout          34\u22ef01.000000001.00000000.7500000-0.27049466 0.13828085-0.03941095-0.03941095-0.188118822 0.115541855 ce91dba8-d4e5-4e1e-a571-c44661e2baca71846976669701L607054RWalk            22\u22ef00.000000000.00000000.0000000 0.00000000-0.01469748-0.09704659-0.09704659 0.027185751 0.000000000 b231dddf-eb88-412e-9da1-aed9be80d10a71810276543807R642585RWalk            12\u22ef00.750000000.33333330.0000000 0.00000000 0.09244198-0.01469748-0.01469748-0.030833284 0.087659556 4f49422d-55ef-45b8-a81b-f3225bc7f58d71810276543807R642585RWalk            01\u22ef00.400000001.00000000.6000000 0.62092984 0.03561571-0.04188323-0.04188323 0.121579634-0.086063251 <p>EXERCISE #5</p> <p>How many pitches until actual run value is preferred to expected run value (when evaluating pitchers)? In other words, how many pitches does it take for actual run value to be a better than expected run value as a predictor of future run value?</p> In\u00a0[\u00a0]: Copied! <pre># Approach 1: Estimating a linear mixed-effects regression model\nmodel &lt;- lme4::lmer(residual_value ~ (1 | pitcher_id), data = data_with_exp_value)\n# Extract the estimated population variance in true talent across pitchers\npopulation_talent_variance_1 &lt;- lme4::VarCorr(model) |&gt;\n  as.data.frame() |&gt;\n  dplyr::filter(grp == \"pitcher_id\") |&gt;\n  with(vcov)\n# Extract the estimated outcome noise variance per pitch\nnoise_variance_per_pitch_1 &lt;- lme4::VarCorr(model) |&gt;\n  as.data.frame() |&gt;\n  dplyr::filter(grp == \"Residual\") |&gt;\n  with(vcov)\n# Calculate the ratio of the variances above\nthreshold_1 &lt;- noise_variance_per_pitch_1 / population_talent_variance_1\n\n# Approach 2: Estimating population variance as done in The Book\npitcher_summary &lt;- data_with_exp_value |&gt;\n  dplyr::group_by(pitcher_id) |&gt;\n  dplyr::summarize(\n    n = dplyr::n(),\n    diff_value = mean(pitch_value - exp_value),\n    var_diff_value = var(pitch_value - exp_value),\n    .groups = \"drop\"\n  )\n# Calculate the variance across pitches for each pitcher, and then average across pitchers\nnoise_variance_per_pitch_2 &lt;- pitcher_summary |&gt;\n  dplyr::filter(n &gt; 100) |&gt;\n  with(weighted.mean(var_diff_value, w = n))\n# Estimate population variance using the function from The Book\npopulation_talent_variance_2 &lt;- estimate_population_variance(\n  observed_value = pitcher_summary$diff_value,    # this is each pitcher's average performance per pitch\n  noise_variance = noise_var / pitcher_summary$n  # this is the noise variance of each pitcher's average performance per pitch\n)\n# Calculate the ratio of the variances above\nthreshold_2 &lt;- noise_variance_per_pitch_2 / population_talent_variance_2\n</pre> # Approach 1: Estimating a linear mixed-effects regression model model &lt;- lme4::lmer(residual_value ~ (1 | pitcher_id), data = data_with_exp_value) # Extract the estimated population variance in true talent across pitchers population_talent_variance_1 &lt;- lme4::VarCorr(model) |&gt;   as.data.frame() |&gt;   dplyr::filter(grp == \"pitcher_id\") |&gt;   with(vcov) # Extract the estimated outcome noise variance per pitch noise_variance_per_pitch_1 &lt;- lme4::VarCorr(model) |&gt;   as.data.frame() |&gt;   dplyr::filter(grp == \"Residual\") |&gt;   with(vcov) # Calculate the ratio of the variances above threshold_1 &lt;- noise_variance_per_pitch_1 / population_talent_variance_1  # Approach 2: Estimating population variance as done in The Book pitcher_summary &lt;- data_with_exp_value |&gt;   dplyr::group_by(pitcher_id) |&gt;   dplyr::summarize(     n = dplyr::n(),     diff_value = mean(pitch_value - exp_value),     var_diff_value = var(pitch_value - exp_value),     .groups = \"drop\"   ) # Calculate the variance across pitches for each pitcher, and then average across pitchers noise_variance_per_pitch_2 &lt;- pitcher_summary |&gt;   dplyr::filter(n &gt; 100) |&gt;   with(weighted.mean(var_diff_value, w = n)) # Estimate population variance using the function from The Book population_talent_variance_2 &lt;- estimate_population_variance(   observed_value = pitcher_summary$diff_value,    # this is each pitcher's average performance per pitch   noise_variance = noise_var / pitcher_summary$n  # this is the noise variance of each pitcher's average performance per pitch ) # Calculate the ratio of the variances above threshold_2 &lt;- noise_variance_per_pitch_2 / population_talent_variance_2"},{"location":"final_ml_project/sequences/","title":"Sequences","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>def load_data(path):\n  df = pd.read_csv(path)\n  return df\n\ndef get_sequences(df):\n\n  df_with_seqs = df.copy().sort_values(['game_id','event_index', 'pitch_number'])\n  df_with_seqs['sequence1_id'] = pd.NA\n  df_with_seqs['sequence2_id'] = pd.NA\n\n  sequence_id = 1\n\n  for i in range(1, len(df_with_seqs)):\n      # if previous pitch and the current pitch belong to the same at-bat\n      if df_with_seqs['event_index'].iloc[i] == df_with_seqs['event_index'].iloc[i-1]:\n\n          df_with_seqs.at[df_with_seqs.index[i], 'sequence1_id'] = sequence_id\n          df_with_seqs.at[df_with_seqs.index[i-1], 'sequence2_id'] = sequence_id\n\n          sequence_id += 1\n\n  print(f'There are {sequence_id} unique two-pair seqeunces of pitches (in 2023).')\n\n  return df_with_seqs\n</pre> def load_data(path):   df = pd.read_csv(path)   return df  def get_sequences(df):    df_with_seqs = df.copy().sort_values(['game_id','event_index', 'pitch_number'])   df_with_seqs['sequence1_id'] = pd.NA   df_with_seqs['sequence2_id'] = pd.NA    sequence_id = 1    for i in range(1, len(df_with_seqs)):       # if previous pitch and the current pitch belong to the same at-bat       if df_with_seqs['event_index'].iloc[i] == df_with_seqs['event_index'].iloc[i-1]:            df_with_seqs.at[df_with_seqs.index[i], 'sequence1_id'] = sequence_id           df_with_seqs.at[df_with_seqs.index[i-1], 'sequence2_id'] = sequence_id            sequence_id += 1    print(f'There are {sequence_id} unique two-pair seqeunces of pitches (in 2023).')    return df_with_seqs In\u00a0[\u00a0]: Copied! <pre>data = load_data('data_with_exp_value.csv')\nseqs = get_sequences(data)\n</pre> data = load_data('data_with_exp_value.csv') seqs = get_sequences(data) <pre>There are 531555 unique two-pair seqeunces of pitches (in 2023).\n</pre> In\u00a0[\u00a0]: Copied! <pre>cols = ['sequence1_id', 'sequence2_id', 'pitch_value', 'linear_weight', 'bat_side', 'pitch_hand',\n        'pitch_type',\t'ax',\t'ay',\t'az',\t'vx0',\t'vy0',\t'vz0', 'x0',\t'z0',\t'extension',\n        'cy',\t't0',\t'bx',\t'by',\t'bz',\t'cx',\t'cz',\t'release_x', 'release_y',\t'release_z',\n        'plate_y',\t'plate_time',\t'plate_x',\t'plate_z',\t'plate_x_line','plate_z_line',\n        'plate_z_gravity',\t'horz_break',\t'vert_break','induced_vert_break',\t'release_speed']\n\nseqs = seqs.reindex(columns=cols)\n</pre> cols = ['sequence1_id', 'sequence2_id', 'pitch_value', 'linear_weight', 'bat_side', 'pitch_hand',         'pitch_type',\t'ax',\t'ay',\t'az',\t'vx0',\t'vy0',\t'vz0', 'x0',\t'z0',\t'extension',         'cy',\t't0',\t'bx',\t'by',\t'bz',\t'cx',\t'cz',\t'release_x', 'release_y',\t'release_z',         'plate_y',\t'plate_time',\t'plate_x',\t'plate_z',\t'plate_x_line','plate_z_line',         'plate_z_gravity',\t'horz_break',\t'vert_break','induced_vert_break',\t'release_speed']  seqs = seqs.reindex(columns=cols) In\u00a0[\u00a0]: Copied! <pre>seqs\n</pre> seqs Out[\u00a0]: sequence1_id sequence2_id pitch_value linear_weight bat_side pitch_hand pitch_type ax ay az ... plate_time plate_x plate_z plate_x_line plate_z_line plate_z_gravity horz_break vert_break induced_vert_break release_speed 629290 &lt;NA&gt; 1 -0.041983 0.474798 R R FF -4.172672 24.453799 -15.239761 ... 0.420644 0.151021 2.949780 0.520180 4.298051 1.451950 -4.429909 -16.179259 17.973953 131.498471 396803 1 2 -0.055163 0.474798 R R SI -14.149811 24.987614 -22.900697 ... 0.419455 0.028925 2.307596 1.273702 4.322200 1.492164 -14.937321 -24.175240 9.785187 131.944829 313723 2 3 0.000000 0.474798 R R SI -13.640156 23.055409 -25.434018 ... 0.412044 -0.366668 2.657300 0.791249 4.816401 2.085481 -13.895001 -25.909214 6.861833 133.267801 184772 3 4 0.000000 0.474798 R R FC 2.864789 20.046111 -30.204211 ... 0.441646 1.049025 1.724296 0.769635 4.669982 1.532582 3.352685 -35.348227 2.300578 124.110400 419650 4 5 0.018436 0.474798 R R SI -17.457082 26.013916 -21.608071 ... 0.409941 0.942128 1.328057 2.408976 3.143696 0.440581 -17.602177 -21.787668 10.649715 134.892646 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 658901 531551 531552 -0.086091 -0.282984 L R FF -10.081499 29.891504 -10.728427 ... 0.383854 -0.495754 2.594549 0.246970 3.384934 1.014905 -8.912690 -9.484615 18.955722 142.927013 683627 531552 &lt;NA&gt; -0.335173 -0.282984 L R FF -13.348995 33.790874 -9.882967 ... 0.386688 -0.040221 3.184765 0.957799 3.923651 1.518504 -11.976237 -8.866641 19.995121 142.094569 710633 &lt;NA&gt; 531553 -0.041983 -0.273826 R R FF -14.531753 34.573387 -6.742605 ... 0.384710 0.247801 3.400763 1.323161 3.899721 1.519117 -12.904326 -5.987493 22.579754 142.944466 696835 531553 531554 -0.055163 -0.273826 R R FF -11.888952 34.248418 -7.802223 ... 0.377172 0.773872 3.058251 1.619525 3.613218 1.324988 -10.147836 -6.659601 20.799158 145.573782 296333 531554 &lt;NA&gt; -0.176780 -0.273826 R R FS -6.041011 28.924074 -26.556782 ... 0.431595 0.864014 2.721738 1.426656 5.195157 2.198939 -6.751701 -29.681031 6.273580 127.432143 <p>714402 rows \u00d7 37 columns</p> In\u00a0[\u00a0]: Copied! <pre>seqs.to_csv('sequences.csv')\n</pre> seqs.to_csv('sequences.csv')"},{"location":"notebooks/sci4ga_environmental_justice_index/","title":"Environmental Justice Index Notebook","text":"In\u00a0[1]: Copied! <pre>## Importing libraries and dependencies\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> ## Importing libraries and dependencies import pandas as pd import geopandas as gpd import numpy as np import matplotlib.pyplot as plt import seaborn as sns <p>The following data set is found at https://screeningtool.geoplatform.gov/en/downloads#3/33.47/-97.5</p> <p>This data is the straight from the federal government's Climate and Economic Justice Screening Tool.</p> <p>Please refer to the codebooks and guides found in the aforementioned link above for more information about the collection of data, how they're organized, and information about individiual datasets and their features.</p> In\u00a0[2]: Copied! <pre>## Importing CEJST shapefile. If you were to go on CEJST site the files are labeled as 'usa.shp', etc.\ndata = gpd.read_file('cejst.shp')\n\n## Filtering data for Georgia (SF column = State)\nga = data[data['SF'] == 'Georgia']\n</pre> ## Importing CEJST shapefile. If you were to go on CEJST site the files are labeled as 'usa.shp', etc. data = gpd.read_file('cejst.shp')  ## Filtering data for Georgia (SF column = State) ga = data[data['SF'] == 'Georgia'] <p>One of the most important columns in the CEJST dataset is 'SN_C', which is a binary indicator indicating whether a census tract is environmentally disadvantaged or not disadvantaged.</p> <p>Below produces a map that shows census tracts in GA that are environmentally disadvantaged (SN_C) according to definition N:  Water, Workforce, Climate, Energy, Transportation, Housing, Pollution, Health.</p> <p>These definition N burdens are assessed based on exceeding a given sub-metric in the dataset (e.g. expected agriculture rate, asthma prevalency, historic underinvestment, etc). Additionally, a census tract must fulfill a low-income threshold to be classified as a 1 (burdened).  The government uses the low-income clause as a proxy for race and thus do not explicitly include race in their classification.</p> <p>To summarize, a census tract is burdened in any one of the definition N categories if it exceeds a sub-metric threshold and low-income status.</p> <p>The CEJST's classification, which is binary (0 or 1), reflects if a census tract is burdened in any one of the definition N areas.</p> <p>Thus, we are somewhat unable to analyze if a geography faces multiple significant burdens.  We want to advance this map to allow for more nuanced and a continuous classification.  Note, we also want to include race as an explicit predictor for our index, which the CEJST does not (as it uses low income as a proxy).</p> In\u00a0[3]: Copied! <pre>ga.plot(column = 'SN_C', legend = True, cmap = 'Reds')\nplt.title('Environmentally Disadvantaged')\nplt.show()\n</pre> ga.plot(column = 'SN_C', legend = True, cmap = 'Reds') plt.title('Environmentally Disadvantaged') plt.show() <p>Below I make a column of min-max normalized total population to be included in index (as areas with more people should probably get more attention)</p> In\u00a0[4]: Copied! <pre>## Creating column for Min-Max scaled total population \n\nga['norm_TPF'] = (ga['TPF'] - ga['TPF'].min()) / (ga['TPF'].max() - ga['TPF'].min())\n</pre> ## Creating column for Min-Max scaled total population   ga['norm_TPF'] = (ga['TPF'] - ga['TPF'].min()) / (ga['TPF'].max() - ga['TPF'].min()) <pre>/Users/jacoblapp/opt/anaconda3/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n</pre> <p>Below, I create a new column for a tract's nonwhite percentage: DM_NW. This will be int64 ranging from 0-1.  I then plot nonwhite percentagers across the state of Georgia to build an intuition of demographics. We see large nonwhite populations in Atlanta, the Black Belt (including Augusta, AMcon, and Columbus), and Southeast Georgia (Savannah, Brunswick).</p> In\u00a0[5]: Copied! <pre>ga['DM_NW'] = 1- ga['DM_W'] # DM_W is % white\nga.plot(column = 'DM_NW', legend = True, cmap = 'Blues')\nplt.title('% Nonwhite')\nplt.show()\n</pre> ga['DM_NW'] = 1- ga['DM_W'] # DM_W is % white ga.plot(column = 'DM_NW', legend = True, cmap = 'Blues') plt.title('% Nonwhite') plt.show() <pre>/Users/jacoblapp/opt/anaconda3/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n</pre> <p>Below are the columns of the shp CEJST file that will be the primary components of the index.</p> <p>They are the 8 definition CEJST burdens: water, climate, energy, transportation, housing, health, pollution, and workforce. These burdens are all binary classifications based on whether they meet their burdens and satisfy a low income requirement as defined by the CEJST.</p> <p>However, we make two changes for our index.  We exclude the CEJST's definition of workforce burden because it does not include many demographic measures.  Instead of their workforce burden classifcation, we replace it with the mean of a tract's linguistic isolation population, low median household income, and high school education attainment, and unemployment. The resulting column will also range from 0 to 1, but will be a continuous classificaion. This new mean workforce development indicator is titled 'wk'</p> <p>Lastly, and importantly, we directly use the nonwhite percentage column defined above as a direct component of the index, entitled 'DM_NW'.  We also include the normalized tract population from earlier.</p> <p>The index itself is the sum of all of these columns.</p> <p>Now, while the summed index is how we approached this problem, further work should investigate the influence individual sub-metrics (diesel particulate, air quality, all of the micro columns that determine the 0s and 1s).  Our approach was to start our index from the CEJST's 0s and 1s groupings (except for workforce), but if we grouped these burdens on a smaller level we could get different results.  Comparing index values across different grouping methods will give insight into our index's accuracy and consistency.</p> In\u00a0[12]: Copied! <pre>our_workforce_columns = ['LIF_PFS', 'LMI_PFS', 'HSEF', 'UF_PFS']\nga['wk'] = ga[our_workforce_columns].mean(axis = 1)\n\nindex_columns = ['DM_NW', 'norm_TPF', 'N_WTR', 'N_CLT', 'N_ENY', 'N_TRN', 'N_HSG', 'N_PLN', 'N_HLTH', 'wk']\nadditional_columns = ['GEOID10', 'SF', 'CF', 'ga_idx']\n\nga['ga_idx'] = np.sum(ga[index_columns], axis =1) # sum of index columns = index\n</pre> our_workforce_columns = ['LIF_PFS', 'LMI_PFS', 'HSEF', 'UF_PFS'] ga['wk'] = ga[our_workforce_columns].mean(axis = 1)  index_columns = ['DM_NW', 'norm_TPF', 'N_WTR', 'N_CLT', 'N_ENY', 'N_TRN', 'N_HSG', 'N_PLN', 'N_HLTH', 'wk'] additional_columns = ['GEOID10', 'SF', 'CF', 'ga_idx']  ga['ga_idx'] = np.sum(ga[index_columns], axis =1) # sum of index columns = index <pre>/Users/jacoblapp/opt/anaconda3/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/Users/jacoblapp/opt/anaconda3/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n</pre> <p>Below is a sliced data\u0192rame of the Georgia CEJST to show the columns we care about for the index:</p> In\u00a0[7]: Copied! <pre>idx_summary = ga[additional_columns + index_columns]\nidx_summary\n</pre> idx_summary = ga[additional_columns + index_columns] idx_summary Out[7]: GEOID10 SF CF ga_idx DM_NW norm_TPF N_WTR N_CLT N_ENY N_TRN N_HSG N_PLN N_HLTH wk 18332 13135050709 Georgia Gwinnett County 1.777250 0.80 0.517250 0 0 0 0 0 0 0 0.4600 18333 13135050314 Georgia Gwinnett County 1.632987 0.80 0.362987 0 0 0 0 0 0 0 0.4700 18334 13135050315 Georgia Gwinnett County 1.445268 0.72 0.237768 0 0 0 0 0 0 0 0.4875 18335 13135050415 Georgia Gwinnett County 1.024707 0.58 0.152207 0 0 0 0 0 0 0 0.2925 18336 13135050425 Georgia Gwinnett County 0.954023 0.46 0.209023 0 0 0 0 0 0 0 0.2850 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 20296 13153020105 Georgia Houston County 1.183742 0.54 0.218742 0 0 0 0 0 0 0 0.4250 20297 13153020106 Georgia Houston County 0.740818 0.35 0.083318 0 0 0 0 0 0 0 0.3075 20298 13153020108 Georgia Houston County 1.053564 0.38 0.223564 0 0 0 0 0 0 0 0.4500 20299 13153020109 Georgia Houston County 1.075692 0.48 0.330692 0 0 0 0 0 0 0 0.2650 20300 13223120603 Georgia Paulding County 2.136418 0.50 0.276418 0 0 0 1 0 0 0 0.3600 <p>1969 rows \u00d7 14 columns</p> <p>The index is created as a new column and is calculated by summing the columns defined in the previous cell.</p> <p>Here is a map of our Georgia EJ index.  The darker values correspond with higher values, meaning that the geography faces more burdens.  We see that similar areas (SW ATL, Columbus, Augusta, Macon, Black Belt, SW &amp; SE GA) have high index scores and higher nonwhite populations (comparing from the earlier NW map)</p> In\u00a0[8]: Copied! <pre>ga.plot(column = 'ga_idx', legend = True, cmap = 'gist_heat_r')\nplt.title('Georgia EJ Index')\nplt.show()\n</pre> ga.plot(column = 'ga_idx', legend = True, cmap = 'gist_heat_r') plt.title('Georgia EJ Index') plt.show() <p>Here are the ten census tracts that face the most environmental burden according to our index:</p> In\u00a0[9]: Copied! <pre>idx_summary.rename(columns = {'GEOID10' : 'Tract ID', 'SF' : 'State', 'CF' : 'County', 'ga_idx' : 'GA EJ Index'}, inplace = True)\nidx_summary.sort_values(by = 'GA EJ Index', ascending = False).head(10).iloc[:,0:4]\n</pre> idx_summary.rename(columns = {'GEOID10' : 'Tract ID', 'SF' : 'State', 'CF' : 'County', 'ga_idx' : 'GA EJ Index'}, inplace = True) idx_summary.sort_values(by = 'GA EJ Index', ascending = False).head(10).iloc[:,0:4] <pre>/var/folders/rb/wdj9db193s70npfrcghpqt340000gn/T/ipykernel_1359/2148117855.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  idx_summary.rename(columns = {'GEOID10' : 'Tract ID', 'SF' : 'State', 'CF' : 'County', 'ga_idx' : 'GA EJ Index'}, inplace = True)\n</pre> Out[9]: Tract ID State County GA EJ Index 19706 13051001200 Georgia Chatham County 7.740723 19761 13121007603 Georgia Fulton County 7.723135 19762 13121007604 Georgia Fulton County 7.711251 18983 13121011000 Georgia Fulton County 6.835983 18553 13245010600 Georgia Richmond County 6.750743 19293 13121004400 Georgia Fulton County 6.740892 19295 13121007400 Georgia Fulton County 6.732537 19251 13185011000 Georgia Lowndes County 6.723643 20204 13095010601 Georgia Dougherty County 6.663931 18924 13121007500 Georgia Fulton County 6.605145 <p>Here is the distrubtion of our index.  We see that there are lots of census tracts with low index values, and not many with high values.  This phenemona is good for us, as it enables us to easily identify a handful of census tracts in serious need.</p> In\u00a0[10]: Copied! <pre>plt.hist(ga.ga_idx)\nplt.title('Georgia EJ Index')\nplt.xlabel('GA EJ Score')\nplt.ylabel('Number of Census Tracts in Georgia')\n</pre> plt.hist(ga.ga_idx) plt.title('Georgia EJ Index') plt.xlabel('GA EJ Score') plt.ylabel('Number of Census Tracts in Georgia') Out[10]: <pre>Text(0, 0.5, 'Number of Census Tracts in Georgia')</pre> <p>Here is a quick plot showing the relationship between a census tract's nonwhite percentage and their index score.  Clearly, there is a positive relationship between the two, highlighting that nonwhite populations face higher environmental justice burdens.</p> In\u00a0[11]: Copied! <pre>ax = sns.regplot(ga,\n    x = 'DM_NW',\n    y = 'ga_idx',\n    scatter_kws={'alpha':0.25},\n    ci = None)\nax.set(xlabel = 'Percent Nonwhite',\n    ylabel = 'GA EJ Index Score')\n</pre> ax = sns.regplot(ga,     x = 'DM_NW',     y = 'ga_idx',     scatter_kws={'alpha':0.25},     ci = None) ax.set(xlabel = 'Percent Nonwhite',     ylabel = 'GA EJ Index Score') Out[11]: <pre>[Text(0.5, 0, 'Percent Nonwhite'), Text(0, 0.5, 'GA EJ Index Score')]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/sci4ga_environmental_justice_index/#importing-libraries-and-data","title":"Importing Libraries and Data\u00b6","text":""},{"location":"notebooks/sci4ga_environmental_justice_index/#cejst-methodology","title":"CEJST Methodology\u00b6","text":""},{"location":"notebooks/sci4ga_environmental_justice_index/#some-quick-data-manipulation-before-index-construction","title":"Some quick data manipulation before index construction...\u00b6","text":""},{"location":"notebooks/sci4ga_environmental_justice_index/#identifying-columms-that-should-be-the-foundational-components-of-the-georgia-specific-ej-index","title":"Identifying columms that should be the foundational components of the Georgia specific EJ index.\u00b6","text":""},{"location":"notebooks/sci4ga_environmental_justice_index/#plotting-and-analyzing-georgia-specific-ej-index","title":"Plotting and Analyzing Georgia-specific EJ Index\u00b6","text":""}]}