{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to My Portfolio","text":""},{"location":"#center-for-research-computing","title":"Center for Research Computing","text":"<ul> <li>Commission Shift Collaboration<ul> <li>Project README</li> <li>StoryMap Link</li> </ul> </li> </ul>"},{"location":"#science-for-georgia","title":"Science for Georgia","text":"<ul> <li>StoryMap Link </li> <li>Environmental Justice Index Notebook</li> </ul>"},{"location":"#coursework","title":"Coursework","text":"<ul> <li>COMP665 Data Viz Network Visualization</li> </ul>"},{"location":"#projects","title":"Projects","text":""},{"location":"#willie-nelson-project","title":"Willie Nelson Project","text":"<ul> <li>Project Overview<ul> <li>Scripts<ul> <li>BOW Python Script</li> <li>Word Counts Python Script</li> <li>Albums Python Script</li> </ul> </li> <li>Albums Text File</li> <li>Career Word Counts CSV</li> <li>Album Word Counts JSON</li> <li>Bag of Words CSV</li> </ul> </li> </ul>"},{"location":"#writings","title":"Writings","text":"<ul> <li>Computer Ethics Final Paper</li> <li>History of Urban Crises Final Paper</li> <li>Public Policy Analysis Final Paper</li> <li>Election Analytics Paper</li> </ul>"},{"location":"crc/readme/","title":"Spatial Studies Lab x Commission Shift","text":""},{"location":"crc/readme/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Directory Structure</li> <li>Setup</li> <li>Usage</li> <li>Scripts</li> <li>Dependencies</li> <li>Contributing</li> <li>License</li> </ol>"},{"location":"crc/readme/#overview","title":"Overview","text":"<p>This project downloads and processes various oil, gas, and water data across Texas. It includes functionality for consolidating datasets including pipelines, wells, and other related information into shapefiles and other data structures using Python scripts.</p>"},{"location":"crc/readme/#directory-structure","title":"Directory Structure","text":"<ul> <li><code>code</code>: Contains Python scripts for various data processing tasks.</li> <li><code>data</code>: Stores data files used or generated by the scripts. (not included in git)</li> <li><code>tests</code>: Contains near-duplicated Python scripts from <code>code</code>, with slight modifications.</li> <li><code>requirements.txt</code>: Lists Python dependencies required to run the project.</li> </ul>"},{"location":"crc/readme/#setup","title":"Setup","text":"<p>To set up this project locally, follow these steps:</p> <ol> <li>Clone the repository:</li> </ol> <p><code>bash    git clone https://github.com/jslapp3/CCSUs.git    cd CCSUs</code></p> <ol> <li> <p>Install dependencies:</p> <p><code>bash pip install -r requirements.txt</code></p> </li> <li> <p>Test the code</p> <p>If you would like to watch the code run on a small subset of all the data, run the main test script. To run all the tests at once, execute the test_main.py script located within the tests/test_code/ directory:</p> </li> </ol> <p><code>bash    python3 tests/test_code/test_main.py</code></p> <pre><code>Note: It is **not** necessary to run the tests before running the main script.\n</code></pre>"},{"location":"crc/readme/#usage","title":"Usage","text":"<p>To run the main script and process the data, use the following command:</p> <p><code>bash    python code/main.py &lt;root_directory&gt;</code></p> <p>Replace  with the path to your root directory where data will be stored.  <p>However, the argument is optional. If main.py is run without specifying, the scripts default to the current directory. If your current directory is a suitable location, then just run:</p> <p><code>bash    python code/main.py</code></p>"},{"location":"crc/readme/#scripts","title":"Scripts","text":"<p><code>Main Scripts</code></p> <ul> <li><code>main.py</code>: The main entry point for the project. It coordinates data processing tasks by calling functions from various modules.</li> </ul> <p><code>Subdirectory Scripts</code></p> <ul> <li> <p><code>Historical Data</code></p> <ul> <li><code>nrc_pipeline.py</code>: Fetches and downloads historical data from the National Response Center</li> <li><code>nrc.py</code>: Processes and merges historical data from the NRC</li> </ul> </li> <li> <p><code>Inactive Wells</code></p> <ul> <li><code>inactive_pipeline.py</code>: Downloads and processes inactive pipeline data from the Texas Railroad Commission</li> </ul> </li> <li> <p><code>Orphan Wells</code></p> <ul> <li><code>orphan_pipeline.py</code>:  Downloads and processes orphan pipeline data from the Texas Railroad Commission</li> </ul> </li> <li> <p><code>RRC Pipelines Wells</code></p> <ul> <li><code>pipelines_pipeline.py</code>: Downloads pipeline data from the Texas Railroad Commission</li> <li><code>merge_pipelines.py</code>: Merges the downloaded pipeline data from the Texas Railroad Commission</li> <li><code>wells_pipeline.py</code>: Downloads well data from the Texas Railroad Commission</li> <li><code>merge_wells.py</code>: Merges the downloaded well data from the Texas Railroad Commission</li> </ul> </li> <li> <p><code>Water Wells</code></p> <ul> <li><code>brac_locs.py</code>: Downloads BRACS well locations data from the Texas Water Development Board.</li> <li><code>groundwater_wells.py</code>: Downloads groundwater wells data from the Texas Water Development Board.</li> <li><code>sdrdb_wells.py</code>: Downloads SDRDB wells data from the Texas Water Development Board.</li> <li><code>move_water_wells.py</code>: Organizes and moves water wells data from the Texas Water Development Board.</li> </ul> </li> </ul>"},{"location":"crc/readme/#dependencies","title":"Dependencies","text":"<p>You need Anaconda (or some way to run Python code) installed on your machine. To run, ensure you have the required packages by installing from requirements.txt (command above).</p>"},{"location":"crc/readme/#contributing","title":"Contributing","text":"<p>If you would like to contribute to this project, please fork the repository and create a pull request with your changes. Ensure that all code adheres to the project's coding standards and passes all tests.</p>"},{"location":"crc/readme/#credits","title":"Credits","text":"<p>This project was made possible by the support of the Diluvial Grant, in partnership with Commission Shift, the Center for Research Computing/Spatial Studies Lab, and Gulf Scholars.</p>"},{"location":"crc/readme/#license","title":"License","text":"<p>This project is licensed under </p> <p> </p>"},{"location":"crc/readme/#disclaimer","title":"Disclaimer","text":"<p>Our website provides information and data compiled from multiple public sources. While we strive to ensure the accuracy and reliability of the data, we cannot guarantee its completeness, timeliness, or accuracy. The information on this website is provided \"as is\" without any warranties of any kind, express or implied. The data and information provided are for general informational purposes only and are not intended to be a substitute for professional advice. Always seek the advice of a qualified professional with any questions you may have regarding a specific issue. Our website may contain links to third-party websites, and we are not responsible for the content, accuracy, or opinions expressed in such websites. Inclusion of any linked website on our site does not imply approval or endorsement by us. In no event shall our website, its owners, affiliates, or contributors be liable for any direct, indirect, incidental, special, or consequential damages, or damages of any kind, including but not limited to, loss of use, data, or profits, arising out of or in connection with the use of this website or the data contained herein. We may update this Disclaimer from time to time by posting the new Disclaimer on this page. Changes are effective when posted. If you have any questions, please contact us at ppowell@commissionshift.org.</p>"},{"location":"notebooks/comp665_data_viz/COMP665_dataviz_network_viz/","title":"COMP665 Data Viz","text":"In\u00a0[1]: Copied! <pre>\"\"\"\nTemplate for week 7 project in Data Visualization\n\nCompute simple graph layouts using optimization and plot community structure\n\"\"\"\n\nimport math\nimport random\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport scipy.optimize as opt\n\nimport community\n</pre> \"\"\" Template for week 7 project in Data Visualization  Compute simple graph layouts using optimization and plot community structure \"\"\"  import math import random import networkx as nx import matplotlib.pyplot as plt import scipy.optimize as opt  import community In\u00a0[2]: Copied! <pre># Resource paths\nPLOTS_PATH = \"plots/\"\nDATA_PATH = \"data/\"\n</pre> # Resource paths PLOTS_PATH = \"plots/\" DATA_PATH = \"data/\" In\u00a0[3]: Copied! <pre>###############################################################################\n# Provided code \n\ndef make_graph(nodes, edges, name=None):\n    \"\"\"\n    Input: list nodes, list edges consisting of tuples of integer node indices\n    optional string name\n    \n    Output: networkx graph\n    \"\"\"\n    \n    grph = nx.Graph()\n    grph.add_nodes_from(nodes)\n    networkx_edges = [[nodes[edge[0]], nodes[edge[1]]] for edge in edges]\n    grph.add_edges_from(networkx_edges)\n    if name:\n        grph.name = name\n    \n    return grph\n</pre> ############################################################################### # Provided code   def make_graph(nodes, edges, name=None):     \"\"\"     Input: list nodes, list edges consisting of tuples of integer node indices     optional string name          Output: networkx graph     \"\"\"          grph = nx.Graph()     grph.add_nodes_from(nodes)     networkx_edges = [[nodes[edge[0]], nodes[edge[1]]] for edge in edges]     grph.add_edges_from(networkx_edges)     if name:         grph.name = name          return grph In\u00a0[4]: Copied! <pre>NODE_RANGE = [-1, 1]\n\ndef random_node_pos():\n    \"\"\"\n    Output: Tuple of random floats in NODE_RANGE\n    \"\"\"\n\n    node_pos = (random.uniform(NODE_RANGE[0], NODE_RANGE[1]),\n                random.uniform(NODE_RANGE[0], NODE_RANGE[1]))\n    return node_pos\n</pre> NODE_RANGE = [-1, 1]  def random_node_pos():     \"\"\"     Output: Tuple of random floats in NODE_RANGE     \"\"\"      node_pos = (random.uniform(NODE_RANGE[0], NODE_RANGE[1]),                 random.uniform(NODE_RANGE[0], NODE_RANGE[1]))     return node_pos In\u00a0[5]: Copied! <pre>def random_layout(grph, seed=None):\n    \"\"\"\n    Input: graph grph, float seed\n    Output: dictionary indexed by nodes whose values are 2D node positions\n    \"\"\"\n    if seed:\n        random.seed(seed)\n        \n    layout = {}\n    for node in grph.nodes():\n        layout[node] = random_node_pos()\n    \n    return layout\n</pre> def random_layout(grph, seed=None):     \"\"\"     Input: graph grph, float seed     Output: dictionary indexed by nodes whose values are 2D node positions     \"\"\"     if seed:         random.seed(seed)              layout = {}     for node in grph.nodes():         layout[node] = random_node_pos()          return layout In\u00a0[6]: Copied! <pre>def plot_graph(grph, layout, title, with_labels=True, node_colors='y', axs=None):\n    \"\"\"\n    Input: graph grph, dictionary layout of 2D node positions, string title\n    optional with_labels, node_colors as defined in draw_networkx\n    optional axes axs\n    \n    Output: matplotlib figure with specified axes updated to\n    include graph drawn using draw_networkx with node outlines being black\n    \"\"\"\n    \n    base_plot = axs is None\n    if base_plot:\n        fig, axs = plt.subplots()\n    else:\n        fig = axs.figure\n    \n    axs.set_title(title)\n    axs.set_xticks([])\n    axs.set_yticks([])\n    axs.set_aspect(\"equal\")\n    fig.tight_layout()\n    \n    nx.draw_networkx(grph, pos=layout, with_labels=with_labels, \n                     node_color=node_colors, cmap=\"terrain\", ax=axs)\n    axs.collections[0].set_edgecolor('k') \n    \n    return fig\n</pre> def plot_graph(grph, layout, title, with_labels=True, node_colors='y', axs=None):     \"\"\"     Input: graph grph, dictionary layout of 2D node positions, string title     optional with_labels, node_colors as defined in draw_networkx     optional axes axs          Output: matplotlib figure with specified axes updated to     include graph drawn using draw_networkx with node outlines being black     \"\"\"          base_plot = axs is None     if base_plot:         fig, axs = plt.subplots()     else:         fig = axs.figure          axs.set_title(title)     axs.set_xticks([])     axs.set_yticks([])     axs.set_aspect(\"equal\")     fig.tight_layout()          nx.draw_networkx(grph, pos=layout, with_labels=with_labels,                       node_color=node_colors, cmap=\"terrain\", ax=axs)     axs.collections[0].set_edgecolor('k')           return fig In\u00a0[7]: notebook_only Copied! <pre>def test_plot_graph():\n    \"\"\" Test graph plottiong code \"\"\"\n    \n    nodes = [1, 2, 3]\n    edges = [(0, 1), (1, 2)]\n    example_1 = make_graph(nodes, edges, name=\"Example 1\")\n    layout = random_layout(example_1, seed=3)\n    plot_graph(example_1, layout, example_1.name + \" - random\")\n    \n    nodes = [\"cat\", \"dog\", \"pig\", \"horse\"]\n    edges = [(0, 1), (1, 2), (0, 2), (2, 3)]\n    example_2 = make_graph(nodes, edges, name=\"Example 2\")\n    layout = random_layout(example_2, seed=4)\n    plot_graph(example_2, layout, example_2.name + \" - random\", node_colors='g')\n   \n    petersen = nx.petersen_graph()\n    layout = random_layout(petersen, seed=5)\n    plot_graph(petersen, layout, petersen.name + \" - random\", with_labels=False)\n    \ntest_plot_graph()\n</pre> def test_plot_graph():     \"\"\" Test graph plottiong code \"\"\"          nodes = [1, 2, 3]     edges = [(0, 1), (1, 2)]     example_1 = make_graph(nodes, edges, name=\"Example 1\")     layout = random_layout(example_1, seed=3)     plot_graph(example_1, layout, example_1.name + \" - random\")          nodes = [\"cat\", \"dog\", \"pig\", \"horse\"]     edges = [(0, 1), (1, 2), (0, 2), (2, 3)]     example_2 = make_graph(nodes, edges, name=\"Example 2\")     layout = random_layout(example_2, seed=4)     plot_graph(example_2, layout, example_2.name + \" - random\", node_colors='g')         petersen = nx.petersen_graph()     layout = random_layout(petersen, seed=5)     plot_graph(petersen, layout, petersen.name + \" - random\", with_labels=False)      test_plot_graph()     In\u00a0[8]: Copied! <pre>###############################################################################\n# Part 1 - Compute energy-based layout for graphs \n\ndef get_node_indices(grph):\n    \"\"\"\n    Input: graph grph\n    \n    Output: Dictionary whose keys are nodes in grph and whose values \n    are corresponding positions of nodes in grph.nodes()\n    \"\"\"\n    graph_dict = {}\n    \n    nodes = list(grph.nodes)\n\n    for node in nodes:\n        graph_dict[node] = nodes.index(node)\n    \n    return graph_dict\n</pre> ############################################################################### # Part 1 - Compute energy-based layout for graphs   def get_node_indices(grph):     \"\"\"     Input: graph grph          Output: Dictionary whose keys are nodes in grph and whose values      are corresponding positions of nodes in grph.nodes()     \"\"\"     graph_dict = {}          nodes = list(grph.nodes)      for node in nodes:         graph_dict[node] = nodes.index(node)          return graph_dict In\u00a0[9]: Copied! <pre>def distance_error(flat_node_pos, path_lengths):\n    \"\"\"\n    Input: 1D numpy array flat_node_pos of the form [x0 y0 x1 y1 ...],\n    nested dictionary path_lengths of path lengths keyed by node indices\n    \n    Output: Sum of squares of differences between path lengths\n    and geometric distances between pairs of nodes (based on values in flat_node_pos) \n    \n    Note: path_lengths will be computed via all_pairs_shortest_path_length()\n    \"\"\"\n    # first unpack the flattened array \n    coord_pairs = []\n    for idx in range(0, (len(flat_node_pos)) - 1, 2):\n        coord_pairs.append((flat_node_pos[idx], flat_node_pos[idx + 1]))\n    \n    # making dictionary with correct index keys to help math\n    coord_dct = {} \n    for idx in range(0, len(coord_pairs)):\n        coord_dct[idx] = coord_pairs[idx]\n    \n    # iterating over all permutations of coordinate points\n    # adding the geometric distances to dictionary\n    geom_dists = {}\n    for point in coord_dct:\n        geom_dists[point] = {}\n        for other_point in coord_dct:\n            geom = math.sqrt(\n                ((coord_dct[other_point][0] - coord_dct[point][0]) ** 2) + \n                ((coord_dct[other_point][1] - coord_dct[point][1]) ** 2)\n                )\n            geom_dists[point][other_point] = geom\n    \n    sq_dist = 0\n    \n    # iterate over final two dictionaries, taking sq dist for every key combo\n\n    for key1 in geom_dists:\n        if key1 in path_lengths:\n            for key2 in geom_dists[key1]:\n                if key2 in path_lengths[key1]:\n                    sq_dist += (geom_dists[key1][key2] - path_lengths[key1][key2]) ** 2\n\n    return sq_dist\n</pre> def distance_error(flat_node_pos, path_lengths):     \"\"\"     Input: 1D numpy array flat_node_pos of the form [x0 y0 x1 y1 ...],     nested dictionary path_lengths of path lengths keyed by node indices          Output: Sum of squares of differences between path lengths     and geometric distances between pairs of nodes (based on values in flat_node_pos)           Note: path_lengths will be computed via all_pairs_shortest_path_length()     \"\"\"     # first unpack the flattened array      coord_pairs = []     for idx in range(0, (len(flat_node_pos)) - 1, 2):         coord_pairs.append((flat_node_pos[idx], flat_node_pos[idx + 1]))          # making dictionary with correct index keys to help math     coord_dct = {}      for idx in range(0, len(coord_pairs)):         coord_dct[idx] = coord_pairs[idx]          # iterating over all permutations of coordinate points     # adding the geometric distances to dictionary     geom_dists = {}     for point in coord_dct:         geom_dists[point] = {}         for other_point in coord_dct:             geom = math.sqrt(                 ((coord_dct[other_point][0] - coord_dct[point][0]) ** 2) +                  ((coord_dct[other_point][1] - coord_dct[point][1]) ** 2)                 )             geom_dists[point][other_point] = geom          sq_dist = 0          # iterate over final two dictionaries, taking sq dist for every key combo      for key1 in geom_dists:         if key1 in path_lengths:             for key2 in geom_dists[key1]:                 if key2 in path_lengths[key1]:                     sq_dist += (geom_dists[key1][key2] - path_lengths[key1][key2]) ** 2      return sq_dist In\u00a0[10]: Copied! <pre>def distance_layout(grph, seed=None):\n    \"\"\"\n    Input: graph grph, optional integer seed\n\n    Output: Dictionary of node positions keyed by nodes\n    and computed using shortest path distances\n\n    Note: The initial guess for opt.minimize() should be generated via n calls\n    to random_node_pos() where n is the number of nodes in grph.\n    \"\"\"\n\n    if seed:\n        random.seed(seed)\n\n    grph_copy = grph.copy()\n\n    indices = get_node_indices(grph_copy)\n    grph_copy = nx.relabel_nodes(grph_copy, indices)\n\n    shortest_paths = dict(nx.all_pairs_shortest_path_length(grph_copy))\n\n    flat_node_pos = []\n    \n    num_nodes = len(grph.nodes())\n    \n    for node in range(num_nodes):\n        pos = random_node_pos()\n        flat_node_pos.extend(pos)\n    \n    result = opt.minimize(distance_error, flat_node_pos, args=(shortest_paths,))\n    \n    node_positions = [tuple(result.x[i:i+2]) for i in range(0, len(result.x), 2)]\n\n    node_positions_dict = {}\n    for node, pos in zip(indices.keys(), node_positions):\n        node_positions_dict[node] = pos\n\n    return node_positions_dict\n</pre> def distance_layout(grph, seed=None):     \"\"\"     Input: graph grph, optional integer seed      Output: Dictionary of node positions keyed by nodes     and computed using shortest path distances      Note: The initial guess for opt.minimize() should be generated via n calls     to random_node_pos() where n is the number of nodes in grph.     \"\"\"      if seed:         random.seed(seed)      grph_copy = grph.copy()      indices = get_node_indices(grph_copy)     grph_copy = nx.relabel_nodes(grph_copy, indices)      shortest_paths = dict(nx.all_pairs_shortest_path_length(grph_copy))      flat_node_pos = []          num_nodes = len(grph.nodes())          for node in range(num_nodes):         pos = random_node_pos()         flat_node_pos.extend(pos)          result = opt.minimize(distance_error, flat_node_pos, args=(shortest_paths,))          node_positions = [tuple(result.x[i:i+2]) for i in range(0, len(result.x), 2)]      node_positions_dict = {}     for node, pos in zip(indices.keys(), node_positions):         node_positions_dict[node] = pos      return node_positions_dict In\u00a0[11]: Copied! <pre>#########################################################################\n# Student code comparing layout methods (peer-graded)\n\ndef plot_spring_vs_distance(grph, with_labels=True, node_colors='y', seed=None):\n    \"\"\"\n    Input: graph grph, optional bool with_labels, optional string node_colors, optional int seed\n    \n    Output: matplotlib figure consisting of side-by-side comparision of \n    grph using spring and distance layouts\n    \"\"\"\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))\n    \n    if seed:\n        random.seed(seed)\n     \n    graph_name = grph.graph.get('name', 'Unnamed Graph')\n    \n    plot_graph(grph, nx.spring_layout(grph, seed=seed), with_labels=with_labels, \n               title=f'Spring Layout ({graph_name})', node_colors=node_colors,\n               axs=ax1)\n    \n    plot_graph(grph, distance_layout(grph, seed=seed), with_labels=with_labels, \n               title=f'Distance Layout ({graph_name})', node_colors=node_colors,\n               axs=ax2)\n    \n    return fig\n</pre> ######################################################################### # Student code comparing layout methods (peer-graded)  def plot_spring_vs_distance(grph, with_labels=True, node_colors='y', seed=None):     \"\"\"     Input: graph grph, optional bool with_labels, optional string node_colors, optional int seed          Output: matplotlib figure consisting of side-by-side comparision of      grph using spring and distance layouts     \"\"\"          fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))          if seed:         random.seed(seed)           graph_name = grph.graph.get('name', 'Unnamed Graph')          plot_graph(grph, nx.spring_layout(grph, seed=seed), with_labels=with_labels,                 title=f'Spring Layout ({graph_name})', node_colors=node_colors,                axs=ax1)          plot_graph(grph, distance_layout(grph, seed=seed), with_labels=with_labels,                 title=f'Distance Layout ({graph_name})', node_colors=node_colors,                axs=ax2)          return fig In\u00a0[12]: notebook_only Copied! <pre>def test_plot_spring_vs_distance():\n    \"\"\" Test plot_spring_vs_distance() \"\"\"\n\n    nodes = [1, 2, 3]\n    edges = [(0, 1), (1, 2)]\n    example_1 = make_graph(nodes, edges, name=\"Example 1\")\n    plot_spring_vs_distance(example_1, seed=2)\n\n    nodes = [\"cat\", \"dog\", \"pig\", \"horse\"]\n    edges = [(0, 1), (1, 2), (0, 2), (2, 3)]\n    example_2 = make_graph(nodes, edges, name=\"Example 2\")\n    plot_spring_vs_distance(example_2, seed=2)\n     \n    petersen = nx.petersen_graph()\n    plot_spring_vs_distance(petersen, False, seed=1)\n\n    dodecahedral = nx.dodecahedral_graph()\n    plot_spring_vs_distance(dodecahedral, False, seed=1)\n    \n    florentine = nx.florentine_families_graph()\n    florentine.name = \"Florentine Graph\"\n    plot_spring_vs_distance(florentine, seed=3)\n    \n    caveman_1 = nx.relaxed_caveman_graph(2, 5, 0.07, seed=3)\n    caveman_1.name = \"Caveman, n=2, s=5, p=0.07\"\n    cliques = [idx // 5 for idx in range(10)]\n    plot_spring_vs_distance(caveman_1, False, cliques, seed=1)\n    \n    caveman_2 = nx.relaxed_caveman_graph(4, 6, 0.07, seed=5)\n    caveman_2.name = \"Caveman, n=4, s=6, p=0.07\"\n    cliques = [idx // 6 for idx in range(24)]\n    plot_spring_vs_distance(caveman_2, False, cliques, seed=1)\n\n\ntest_plot_spring_vs_distance()\n</pre> def test_plot_spring_vs_distance():     \"\"\" Test plot_spring_vs_distance() \"\"\"      nodes = [1, 2, 3]     edges = [(0, 1), (1, 2)]     example_1 = make_graph(nodes, edges, name=\"Example 1\")     plot_spring_vs_distance(example_1, seed=2)      nodes = [\"cat\", \"dog\", \"pig\", \"horse\"]     edges = [(0, 1), (1, 2), (0, 2), (2, 3)]     example_2 = make_graph(nodes, edges, name=\"Example 2\")     plot_spring_vs_distance(example_2, seed=2)           petersen = nx.petersen_graph()     plot_spring_vs_distance(petersen, False, seed=1)      dodecahedral = nx.dodecahedral_graph()     plot_spring_vs_distance(dodecahedral, False, seed=1)          florentine = nx.florentine_families_graph()     florentine.name = \"Florentine Graph\"     plot_spring_vs_distance(florentine, seed=3)          caveman_1 = nx.relaxed_caveman_graph(2, 5, 0.07, seed=3)     caveman_1.name = \"Caveman, n=2, s=5, p=0.07\"     cliques = [idx // 5 for idx in range(10)]     plot_spring_vs_distance(caveman_1, False, cliques, seed=1)          caveman_2 = nx.relaxed_caveman_graph(4, 6, 0.07, seed=5)     caveman_2.name = \"Caveman, n=4, s=6, p=0.07\"     cliques = [idx // 6 for idx in range(24)]     plot_spring_vs_distance(caveman_2, False, cliques, seed=1)   test_plot_spring_vs_distance() In\u00a0[13]: Copied! <pre>##########################################################\n# Part 2 - Compute and plot communities\n\ndef get_communities(grph, seed=None):\n    \"\"\"\n    Input: graph grph, optional int seed\n    \n    Output: List of integers indicating the community for each\n    corresponding node in grph\n    \"\"\"\n    \n    if seed:\n        random.seed(seed)\n    \n    return list(community.best_partition(grph, random_state=seed).values())\n</pre> ########################################################## # Part 2 - Compute and plot communities  def get_communities(grph, seed=None):     \"\"\"     Input: graph grph, optional int seed          Output: List of integers indicating the community for each     corresponding node in grph     \"\"\"          if seed:         random.seed(seed)          return list(community.best_partition(grph, random_state=seed).values()) In\u00a0[14]: Copied! <pre>def plot_caveman_communities(num_cliques, clique_size, prob, seed=None):\n    \"\"\"\n    Input: integers num_cliques, clique_sizes, floats prob, seed\n    \n    Output: matplotlib figure containing plot of communities in\n    relaxed caveman graphs using spring layout\n    \"\"\"\n    \n    if seed:\n        random.seed(seed)\n    \n    fig, axs = plt.subplots()\n    \n    caveman_graph = nx.relaxed_caveman_graph(l=num_cliques, k=clique_size, p=prob, seed=seed)\n\n    caveman_community = get_communities(caveman_graph, seed=seed)\n    \n    plot_graph(caveman_graph, nx.spring_layout(caveman_graph, seed=seed),\n               title=f'Caveman Communities with {num_cliques} Cliques of Size {clique_size}',\n               axs=axs, with_labels=caveman_community,\n               node_colors=caveman_community)\n    \n    return fig\n</pre> def plot_caveman_communities(num_cliques, clique_size, prob, seed=None):     \"\"\"     Input: integers num_cliques, clique_sizes, floats prob, seed          Output: matplotlib figure containing plot of communities in     relaxed caveman graphs using spring layout     \"\"\"          if seed:         random.seed(seed)          fig, axs = plt.subplots()          caveman_graph = nx.relaxed_caveman_graph(l=num_cliques, k=clique_size, p=prob, seed=seed)      caveman_community = get_communities(caveman_graph, seed=seed)          plot_graph(caveman_graph, nx.spring_layout(caveman_graph, seed=seed),                title=f'Caveman Communities with {num_cliques} Cliques of Size {clique_size}',                axs=axs, with_labels=caveman_community,                node_colors=caveman_community)          return fig In\u00a0[15]: notebook_only Copied! <pre>def test_plot_caveman_communities():\n    \"\"\" Test plot_caveman_communities \"\"\"\n    \n    plot_caveman_communities(2, 5, 0.07, seed=3)\n    plot_caveman_communities(5, 6, 0.12, seed=7)\n    \n    # Note larger clique_size makes community detection more reliable\n    plot_caveman_communities(8, 5, 0.12, seed=3)\n    plot_caveman_communities(8, 4, 0.12, seed=7) \n        \ntest_plot_caveman_communities()\n</pre> def test_plot_caveman_communities():     \"\"\" Test plot_caveman_communities \"\"\"          plot_caveman_communities(2, 5, 0.07, seed=3)     plot_caveman_communities(5, 6, 0.12, seed=7)          # Note larger clique_size makes community detection more reliable     plot_caveman_communities(8, 5, 0.12, seed=3)     plot_caveman_communities(8, 4, 0.12, seed=7)           test_plot_caveman_communities() In\u00a0[28]: Copied! <pre>def plot_facebook_communities(seed=None):\n    \"\"\"\n    Input: optional int seed\n    \n    Output: matplotlib figure consisting of communities for Facebook ego network from\n    https://blog.dominodatalab.com/social-network-analysis-with-networkx/\n    \"\"\"\n    \n    if seed:\n        random.seed(seed)\n    \n    path = DATA_PATH + 'facebook_combined.txt'\n    \n    fb_graph = nx.read_edgelist(path)\n    \n    fb_communities = get_communities(fb_graph, seed=seed)\n    \n    plt.figure(figsize=(16, 12))\n    \n    nx.draw_networkx(fb_graph, pos=nx.spring_layout(fb_graph),\n                     with_labels=False, node_color=fb_communities, node_size=35)\n    \n    plt.title('Facebook Ego Network')\n    \n    plt.show()\n    \n    return plt\n</pre> def plot_facebook_communities(seed=None):     \"\"\"     Input: optional int seed          Output: matplotlib figure consisting of communities for Facebook ego network from     https://blog.dominodatalab.com/social-network-analysis-with-networkx/     \"\"\"          if seed:         random.seed(seed)          path = DATA_PATH + 'facebook_combined.txt'          fb_graph = nx.read_edgelist(path)          fb_communities = get_communities(fb_graph, seed=seed)          plt.figure(figsize=(16, 12))          nx.draw_networkx(fb_graph, pos=nx.spring_layout(fb_graph),                      with_labels=False, node_color=fb_communities, node_size=35)          plt.title('Facebook Ego Network')          plt.show()          return plt In\u00a0[29]: notebook_only Copied! <pre>def test_plot_facebook_communities():\n    \"\"\" Call plot_face_communities() \"\"\"\n    \n    plot_facebook_communities(seed=3)\n    \ntest_plot_facebook_communities()\n</pre> def test_plot_facebook_communities():     \"\"\" Call plot_face_communities() \"\"\"          plot_facebook_communities(seed=3)      test_plot_facebook_communities()"},{"location":"notebooks/comp665_data_viz/COMP665_dataviz_network_viz/#question-1-optional","title":"Question 1 (optional)\u00b6","text":"<p>Based on these examples, what are the advantages/disadvantages of the two layout methods?</p> <p>Insert answer here.</p>"},{"location":"notebooks/comp665_data_viz/COMP665_dataviz_network_viz/#question-2-optional","title":"Question 2 (optional)\u00b6","text":"<p>Based on these examples, how well does the get_community() method detect communities in a graph?</p> <p>Insert answer here.</p>"},{"location":"notebooks/sci4ga/sci4ga_environmental_justice_index/","title":"Environmental Justice Index Notebook","text":"In\u00a0[1]: Copied! <pre>## Importing libraries and dependencies\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n</pre> ## Importing libraries and dependencies import pandas as pd import geopandas as gpd import numpy as np import matplotlib.pyplot as plt import seaborn as sns <p>The following data set is found at https://screeningtool.geoplatform.gov/en/downloads#3/33.47/-97.5</p> <p>This data is the straight from the federal government's Climate and Economic Justice Screening Tool.</p> <p>Please refer to the codebooks and guides found in the aforementioned link above for more information about the collection of data, how they're organized, and information about individiual datasets and their features.</p> In\u00a0[2]: Copied! <pre>## Importing CEJST shapefile. If you were to go on CEJST site the files are labeled as 'usa.shp', etc.\ndata = gpd.read_file('cejst.shp')\n\n## Filtering data for Georgia (SF column = State)\nga = data[data['SF'] == 'Georgia']\n</pre> ## Importing CEJST shapefile. If you were to go on CEJST site the files are labeled as 'usa.shp', etc. data = gpd.read_file('cejst.shp')  ## Filtering data for Georgia (SF column = State) ga = data[data['SF'] == 'Georgia'] <p>One of the most important columns in the CEJST dataset is 'SN_C', which is a binary indicator indicating whether a census tract is environmentally disadvantaged or not disadvantaged.</p> <p>Below produces a map that shows census tracts in GA that are environmentally disadvantaged (SN_C) according to definition N:  Water, Workforce, Climate, Energy, Transportation, Housing, Pollution, Health.</p> <p>These definition N burdens are assessed based on exceeding a given sub-metric in the dataset (e.g. expected agriculture rate, asthma prevalency, historic underinvestment, etc). Additionally, a census tract must fulfill a low-income threshold to be classified as a 1 (burdened).  The government uses the low-income clause as a proxy for race and thus do not explicitly include race in their classification.</p> <p>To summarize, a census tract is burdened in any one of the definition N categories if it exceeds a sub-metric threshold and low-income status.</p> <p>The CEJST's classification, which is binary (0 or 1), reflects if a census tract is burdened in any one of the definition N areas.</p> <p>Thus, we are somewhat unable to analyze if a geography faces multiple significant burdens.  We want to advance this map to allow for more nuanced and a continuous classification.  Note, we also want to include race as an explicit predictor for our index, which the CEJST does not (as it uses low income as a proxy).</p> In\u00a0[3]: Copied! <pre>ga.plot(column = 'SN_C', legend = True, cmap = 'Reds')\nplt.title('Environmentally Disadvantaged')\nplt.show()\n</pre> ga.plot(column = 'SN_C', legend = True, cmap = 'Reds') plt.title('Environmentally Disadvantaged') plt.show() <p>Below I make a column of min-max normalized total population to be included in index (as areas with more people should probably get more attention)</p> In\u00a0[4]: Copied! <pre>## Creating column for Min-Max scaled total population \n\nga['norm_TPF'] = (ga['TPF'] - ga['TPF'].min()) / (ga['TPF'].max() - ga['TPF'].min())\n</pre> ## Creating column for Min-Max scaled total population   ga['norm_TPF'] = (ga['TPF'] - ga['TPF'].min()) / (ga['TPF'].max() - ga['TPF'].min()) <pre>/Users/jacoblapp/opt/anaconda3/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n</pre> <p>Below, I create a new column for a tract's nonwhite percentage: DM_NW. This will be int64 ranging from 0-1.  I then plot nonwhite percentagers across the state of Georgia to build an intuition of demographics. We see large nonwhite populations in Atlanta, the Black Belt (including Augusta, AMcon, and Columbus), and Southeast Georgia (Savannah, Brunswick).</p> In\u00a0[5]: Copied! <pre>ga['DM_NW'] = 1- ga['DM_W'] # DM_W is % white\nga.plot(column = 'DM_NW', legend = True, cmap = 'Blues')\nplt.title('% Nonwhite')\nplt.show()\n</pre> ga['DM_NW'] = 1- ga['DM_W'] # DM_W is % white ga.plot(column = 'DM_NW', legend = True, cmap = 'Blues') plt.title('% Nonwhite') plt.show() <pre>/Users/jacoblapp/opt/anaconda3/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n</pre> <p>Below are the columns of the shp CEJST file that will be the primary components of the index.</p> <p>They are the 8 definition CEJST burdens: water, climate, energy, transportation, housing, health, pollution, and workforce. These burdens are all binary classifications based on whether they meet their burdens and satisfy a low income requirement as defined by the CEJST.</p> <p>However, we make two changes for our index.  We exclude the CEJST's definition of workforce burden because it does not include many demographic measures.  Instead of their workforce burden classifcation, we replace it with the mean of a tract's linguistic isolation population, low median household income, and high school education attainment, and unemployment. The resulting column will also range from 0 to 1, but will be a continuous classificaion. This new mean workforce development indicator is titled 'wk'</p> <p>Lastly, and importantly, we directly use the nonwhite percentage column defined above as a direct component of the index, entitled 'DM_NW'.  We also include the normalized tract population from earlier.</p> <p>The index itself is the sum of all of these columns.</p> <p>Now, while the summed index is how we approached this problem, further work should investigate the influence individual sub-metrics (diesel particulate, air quality, all of the micro columns that determine the 0s and 1s).  Our approach was to start our index from the CEJST's 0s and 1s groupings (except for workforce), but if we grouped these burdens on a smaller level we could get different results.  Comparing index values across different grouping methods will give insight into our index's accuracy and consistency.</p> In\u00a0[12]: Copied! <pre>our_workforce_columns = ['LIF_PFS', 'LMI_PFS', 'HSEF', 'UF_PFS']\nga['wk'] = ga[our_workforce_columns].mean(axis = 1)\n\nindex_columns = ['DM_NW', 'norm_TPF', 'N_WTR', 'N_CLT', 'N_ENY', 'N_TRN', 'N_HSG', 'N_PLN', 'N_HLTH', 'wk']\nadditional_columns = ['GEOID10', 'SF', 'CF', 'ga_idx']\n\nga['ga_idx'] = np.sum(ga[index_columns], axis =1) # sum of index columns = index\n</pre> our_workforce_columns = ['LIF_PFS', 'LMI_PFS', 'HSEF', 'UF_PFS'] ga['wk'] = ga[our_workforce_columns].mean(axis = 1)  index_columns = ['DM_NW', 'norm_TPF', 'N_WTR', 'N_CLT', 'N_ENY', 'N_TRN', 'N_HSG', 'N_PLN', 'N_HLTH', 'wk'] additional_columns = ['GEOID10', 'SF', 'CF', 'ga_idx']  ga['ga_idx'] = np.sum(ga[index_columns], axis =1) # sum of index columns = index <pre>/Users/jacoblapp/opt/anaconda3/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n/Users/jacoblapp/opt/anaconda3/lib/python3.9/site-packages/geopandas/geodataframe.py:1443: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  super().__setitem__(key, value)\n</pre> <p>Below is a sliced data\u0192rame of the Georgia CEJST to show the columns we care about for the index:</p> In\u00a0[7]: Copied! <pre>idx_summary = ga[additional_columns + index_columns]\nidx_summary\n</pre> idx_summary = ga[additional_columns + index_columns] idx_summary Out[7]: GEOID10 SF CF ga_idx DM_NW norm_TPF N_WTR N_CLT N_ENY N_TRN N_HSG N_PLN N_HLTH wk 18332 13135050709 Georgia Gwinnett County 1.777250 0.80 0.517250 0 0 0 0 0 0 0 0.4600 18333 13135050314 Georgia Gwinnett County 1.632987 0.80 0.362987 0 0 0 0 0 0 0 0.4700 18334 13135050315 Georgia Gwinnett County 1.445268 0.72 0.237768 0 0 0 0 0 0 0 0.4875 18335 13135050415 Georgia Gwinnett County 1.024707 0.58 0.152207 0 0 0 0 0 0 0 0.2925 18336 13135050425 Georgia Gwinnett County 0.954023 0.46 0.209023 0 0 0 0 0 0 0 0.2850 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 20296 13153020105 Georgia Houston County 1.183742 0.54 0.218742 0 0 0 0 0 0 0 0.4250 20297 13153020106 Georgia Houston County 0.740818 0.35 0.083318 0 0 0 0 0 0 0 0.3075 20298 13153020108 Georgia Houston County 1.053564 0.38 0.223564 0 0 0 0 0 0 0 0.4500 20299 13153020109 Georgia Houston County 1.075692 0.48 0.330692 0 0 0 0 0 0 0 0.2650 20300 13223120603 Georgia Paulding County 2.136418 0.50 0.276418 0 0 0 1 0 0 0 0.3600 <p>1969 rows \u00d7 14 columns</p> <p>The index is created as a new column and is calculated by summing the columns defined in the previous cell.</p> <p>Here is a map of our Georgia EJ index.  The darker values correspond with higher values, meaning that the geography faces more burdens.  We see that similar areas (SW ATL, Columbus, Augusta, Macon, Black Belt, SW &amp; SE GA) have high index scores and higher nonwhite populations (comparing from the earlier NW map)</p> In\u00a0[8]: Copied! <pre>ga.plot(column = 'ga_idx', legend = True, cmap = 'gist_heat_r')\nplt.title('Georgia EJ Index')\nplt.show()\n</pre> ga.plot(column = 'ga_idx', legend = True, cmap = 'gist_heat_r') plt.title('Georgia EJ Index') plt.show() <p>Here are the ten census tracts that face the most environmental burden according to our index:</p> In\u00a0[9]: Copied! <pre>idx_summary.rename(columns = {'GEOID10' : 'Tract ID', 'SF' : 'State', 'CF' : 'County', 'ga_idx' : 'GA EJ Index'}, inplace = True)\nidx_summary.sort_values(by = 'GA EJ Index', ascending = False).head(10).iloc[:,0:4]\n</pre> idx_summary.rename(columns = {'GEOID10' : 'Tract ID', 'SF' : 'State', 'CF' : 'County', 'ga_idx' : 'GA EJ Index'}, inplace = True) idx_summary.sort_values(by = 'GA EJ Index', ascending = False).head(10).iloc[:,0:4] <pre>/var/folders/rb/wdj9db193s70npfrcghpqt340000gn/T/ipykernel_1359/2148117855.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  idx_summary.rename(columns = {'GEOID10' : 'Tract ID', 'SF' : 'State', 'CF' : 'County', 'ga_idx' : 'GA EJ Index'}, inplace = True)\n</pre> Out[9]: Tract ID State County GA EJ Index 19706 13051001200 Georgia Chatham County 7.740723 19761 13121007603 Georgia Fulton County 7.723135 19762 13121007604 Georgia Fulton County 7.711251 18983 13121011000 Georgia Fulton County 6.835983 18553 13245010600 Georgia Richmond County 6.750743 19293 13121004400 Georgia Fulton County 6.740892 19295 13121007400 Georgia Fulton County 6.732537 19251 13185011000 Georgia Lowndes County 6.723643 20204 13095010601 Georgia Dougherty County 6.663931 18924 13121007500 Georgia Fulton County 6.605145 <p>Here is the distrubtion of our index.  We see that there are lots of census tracts with low index values, and not many with high values.  This phenemona is good for us, as it enables us to easily identify a handful of census tracts in serious need.</p> In\u00a0[10]: Copied! <pre>plt.hist(ga.ga_idx)\nplt.title('Georgia EJ Index')\nplt.xlabel('GA EJ Score')\nplt.ylabel('Number of Census Tracts in Georgia')\n</pre> plt.hist(ga.ga_idx) plt.title('Georgia EJ Index') plt.xlabel('GA EJ Score') plt.ylabel('Number of Census Tracts in Georgia') Out[10]: <pre>Text(0, 0.5, 'Number of Census Tracts in Georgia')</pre> <p>Here is a quick plot showing the relationship between a census tract's nonwhite percentage and their index score.  Clearly, there is a positive relationship between the two, highlighting that nonwhite populations face higher environmental justice burdens.</p> In\u00a0[11]: Copied! <pre>ax = sns.regplot(ga,\n    x = 'DM_NW',\n    y = 'ga_idx',\n    scatter_kws={'alpha':0.25},\n    ci = None)\nax.set(xlabel = 'Percent Nonwhite',\n    ylabel = 'GA EJ Index Score')\n</pre> ax = sns.regplot(ga,     x = 'DM_NW',     y = 'ga_idx',     scatter_kws={'alpha':0.25},     ci = None) ax.set(xlabel = 'Percent Nonwhite',     ylabel = 'GA EJ Index Score') Out[11]: <pre>[Text(0.5, 0, 'Percent Nonwhite'), Text(0, 0.5, 'GA EJ Index Score')]</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/sci4ga/sci4ga_environmental_justice_index/#importing-libraries-and-data","title":"Importing Libraries and Data\u00b6","text":""},{"location":"notebooks/sci4ga/sci4ga_environmental_justice_index/#cejst-methodology","title":"CEJST Methodology\u00b6","text":""},{"location":"notebooks/sci4ga/sci4ga_environmental_justice_index/#some-quick-data-manipulation-before-index-construction","title":"Some quick data manipulation before index construction...\u00b6","text":""},{"location":"notebooks/sci4ga/sci4ga_environmental_justice_index/#identifying-columms-that-should-be-the-foundational-components-of-the-georgia-specific-ej-index","title":"Identifying columms that should be the foundational components of the Georgia specific EJ index.\u00b6","text":""},{"location":"notebooks/sci4ga/sci4ga_environmental_justice_index/#plotting-and-analyzing-georgia-specific-ej-index","title":"Plotting and Analyzing Georgia-specific EJ Index\u00b6","text":""},{"location":"willie-nelson-project/","title":"willie-nelson-project","text":"<p>Textual and Spatial Analyses of Big Bill</p>"},{"location":"willie-nelson-project/albums/","title":"Albums Script","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport lyricsgenius\n</pre> import os import lyricsgenius In\u00a0[\u00a0]: Copied! <pre># Initialize the Genius API\ngenius = lyricsgenius.Genius('StYjV27r-2epV0-KBXDLhtAPZ8qI7qvwPFR0_GFZkKcFkJYfmoVXXPuA58WrT_dD')\n</pre> # Initialize the Genius API genius = lyricsgenius.Genius('StYjV27r-2epV0-KBXDLhtAPZ8qI7qvwPFR0_GFZkKcFkJYfmoVXXPuA58WrT_dD') In\u00a0[\u00a0]: Copied! <pre># Read album names from the file\nwith open('albums.txt', 'r') as f:\n    albums_raw = f.readlines()\n</pre> # Read album names from the file with open('albums.txt', 'r') as f:     albums_raw = f.readlines() In\u00a0[\u00a0]: Copied! <pre># Strip whitespace from each album name\nALBUMS = [album.strip() for album in albums_raw]\n</pre> # Strip whitespace from each album name ALBUMS = [album.strip() for album in albums_raw] In\u00a0[\u00a0]: Copied! <pre># Create the data directory if it doesn't exist\nos.makedirs('data', exist_ok=True)\n</pre> # Create the data directory if it doesn't exist os.makedirs('data', exist_ok=True) In\u00a0[\u00a0]: Copied! <pre>DATA_PATH = 'data'\n</pre> DATA_PATH = 'data' In\u00a0[\u00a0]: Copied! <pre>for album_name in ALBUMS:\n    album = genius.search_album(album_name, \"Willie Nelson\")\n    \n    # Create a clean album directory name\n    album_dir = os.path.join(DATA_PATH, album_name.replace(\" \", \"_\").replace(\"/\", \"_\"))\n    os.makedirs(album_dir, exist_ok=True)\n\n    for song in album.tracks:\n        # Fetch the lyrics\n        song_lyrics = song.song.lyrics\n\n        # Clean the song title to use it as a file name\n        clean_title = song.song.title.replace(\" \", \"_\").replace(\"/\", \"_\")\n\n        # Write lyrics to a TXT file in the album directory\n        file_path = os.path.join(album_dir, f\"{clean_title}.txt\")\n        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(song_lyrics)\n\n        print(f\"Lyrics for {song.song.title} saved in {album_dir}.\")\n</pre> for album_name in ALBUMS:     album = genius.search_album(album_name, \"Willie Nelson\")          # Create a clean album directory name     album_dir = os.path.join(DATA_PATH, album_name.replace(\" \", \"_\").replace(\"/\", \"_\"))     os.makedirs(album_dir, exist_ok=True)      for song in album.tracks:         # Fetch the lyrics         song_lyrics = song.song.lyrics          # Clean the song title to use it as a file name         clean_title = song.song.title.replace(\" \", \"_\").replace(\"/\", \"_\")          # Write lyrics to a TXT file in the album directory         file_path = os.path.join(album_dir, f\"{clean_title}.txt\")         with open(file_path, \"w\", encoding=\"utf-8\") as file:             file.write(song_lyrics)          print(f\"Lyrics for {song.song.title} saved in {album_dir}.\")"},{"location":"willie-nelson-project/bow/","title":"Bag of Words Script","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[\u00a0]: Copied! <pre>def jsons_df(json_path):\n    json_df = pd.read_json(json_path)\n    json_df.fillna(0, inplace=True)\n    json_df = json_df.astype(int)\n    return json_df\n</pre> def jsons_df(json_path):     json_df = pd.read_json(json_path)     json_df.fillna(0, inplace=True)     json_df = json_df.astype(int)     return json_df In\u00a0[\u00a0]: Copied! <pre>print('\\nBag of Words for Each Album:\\n')\nalbum_bow = jsons_df('album_word_counts.json')\nalbum_bow.to_csv('bag_of_words.csv')\nprint(album_bow)\n</pre> print('\\nBag of Words for Each Album:\\n') album_bow = jsons_df('album_word_counts.json') album_bow.to_csv('bag_of_words.csv') print(album_bow) In\u00a0[\u00a0]: Copied! <pre>print('\\nTotal \u00c7areer Word Count\\n')\nfull_word_counts = pd.DataFrame(album_bow.sum(axis=1), columns=['count']).sort_values('count', ascending=False)\nfull_word_counts.to_csv('career_word_counts.csv')\nprint(full_word_counts)\n</pre> print('\\nTotal \u00c7areer Word Count\\n') full_word_counts = pd.DataFrame(album_bow.sum(axis=1), columns=['count']).sort_values('count', ascending=False) full_word_counts.to_csv('career_word_counts.csv') print(full_word_counts)"},{"location":"willie-nelson-project/word_counts/","title":"Word Counts Script","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport re\nimport json\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom pprint import pprint\n</pre> import os import re import json import nltk from nltk.corpus import stopwords from nltk.stem import PorterStemmer from nltk.stem import WordNetLemmatizer from pprint import pprint In\u00a0[\u00a0]: Copied! <pre>nltk.download('wordnet')\nnltk.download('stopwords')\n</pre> nltk.download('wordnet') nltk.download('stopwords') In\u00a0[\u00a0]: Copied! <pre>DATA_PATH = 'data'\n</pre> DATA_PATH = 'data' In\u00a0[\u00a0]: Copied! <pre>def get_song_list(rel_song_path):\n    \"\"\"\n    Inputs -- str: path to song\n    Outputs -- list of stemmed and lemmatized words in the song, with stop words and specified tags removed\n    \"\"\"\n    # Compile regex patterns\n    tags_pattern = re.compile(r'[\\*\\[\\]]')  # Matches *, [, or ]\n    unwanted_pattern = re.compile(r'[^\\w\\s]|\\d')  # Matches special characters (excluding letters and digits) and digits\n\n    # Initialize stemmer and lemmatizer\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    \n    stop_words = set(stopwords.words('english'))\n\n    weird_words = {'willie', 'nelson', 'ticket', 'anymoreembed', '$19you', \n                   'might', 'also', 'like3embed', 'contributorswhiskey', 'tickets', 'liveget', \n                   'roundsee'}\n\n    excluded_words = stop_words.union(weird_words)\n\n    word_list = []\n    try:\n        with open(rel_song_path, 'r') as f:\n            lyrics = f.readlines()  # Read all lines from the file\n            # Process each line to filter out stop words, tags, unwanted patterns, and words with specific prefixes\n            for line in lyrics:\n                words = line.lower().split()  # Split the line into words\n                # Filter out words that are stop words, match the tags pattern, unwanted patterns, or start with specific prefixes\n                filtered_words = [word for word in words if word \n                                  not in excluded_words and\n                                  not tags_pattern.search(word) and\n                                  not unwanted_pattern.search(word) and\n                                  not word.startswith('contribut')]\n                \n                # Stem and lemmatize the filtered words\n                #stemmed_lemmas = [lemmatizer.lemmatize(stemmer.stem(word)) for word in filtered_words]\n                word_list.extend(filtered_words)  # Add stemmed and lemmatized words to the list\n    except FileNotFoundError:\n        print(f\"Error: The file at {rel_song_path} does not exist.\")\n    except IOError as e:\n        print(f\"Error reading file {rel_song_path}: {e}\")\n\n    return word_list\n</pre> def get_song_list(rel_song_path):     \"\"\"     Inputs -- str: path to song     Outputs -- list of stemmed and lemmatized words in the song, with stop words and specified tags removed     \"\"\"     # Compile regex patterns     tags_pattern = re.compile(r'[\\*\\[\\]]')  # Matches *, [, or ]     unwanted_pattern = re.compile(r'[^\\w\\s]|\\d')  # Matches special characters (excluding letters and digits) and digits      # Initialize stemmer and lemmatizer     stemmer = PorterStemmer()     lemmatizer = WordNetLemmatizer()          stop_words = set(stopwords.words('english'))      weird_words = {'willie', 'nelson', 'ticket', 'anymoreembed', '$19you',                     'might', 'also', 'like3embed', 'contributorswhiskey', 'tickets', 'liveget',                     'roundsee'}      excluded_words = stop_words.union(weird_words)      word_list = []     try:         with open(rel_song_path, 'r') as f:             lyrics = f.readlines()  # Read all lines from the file             # Process each line to filter out stop words, tags, unwanted patterns, and words with specific prefixes             for line in lyrics:                 words = line.lower().split()  # Split the line into words                 # Filter out words that are stop words, match the tags pattern, unwanted patterns, or start with specific prefixes                 filtered_words = [word for word in words if word                                    not in excluded_words and                                   not tags_pattern.search(word) and                                   not unwanted_pattern.search(word) and                                   not word.startswith('contribut')]                                  # Stem and lemmatize the filtered words                 #stemmed_lemmas = [lemmatizer.lemmatize(stemmer.stem(word)) for word in filtered_words]                 word_list.extend(filtered_words)  # Add stemmed and lemmatized words to the list     except FileNotFoundError:         print(f\"Error: The file at {rel_song_path} does not exist.\")     except IOError as e:         print(f\"Error reading file {rel_song_path}: {e}\")      return word_list In\u00a0[\u00a0]: Copied! <pre>def get_song_word_counts(song_list):\n    \"\"\"\n    Inputs -- list of words from a song\n    Outputs -- dictionary with word counts\n    \"\"\"\n    counts = {}\n    for word in song_list:\n        if word not in counts:\n            counts[word] = 1\n        else:\n            counts[word] += 1\n    return counts\n</pre> def get_song_word_counts(song_list):     \"\"\"     Inputs -- list of words from a song     Outputs -- dictionary with word counts     \"\"\"     counts = {}     for word in song_list:         if word not in counts:             counts[word] = 1         else:             counts[word] += 1     return counts In\u00a0[\u00a0]: Copied! <pre>def get_album_word_counts(album_dir):\n    \"\"\"\n    Inputs -- str: path to album directory\n    Outputs -- dictionary with word counts for the entire album\n    \"\"\"\n    album_word_count = {}\n\n    for root, dirs, files in os.walk(album_dir):\n        for file in files:\n            if file.endswith('.txt'):  # Assuming song files are in .txt format\n                song_path = os.path.join(root, file)\n                song_list = get_song_list(song_path)\n                word_counts = get_song_word_counts(song_list)\n\n                # Aggregate word counts for the album\n                for word, count in word_counts.items():\n                    if word not in album_word_count:\n                        album_word_count[word] = count\n                    else:\n                        album_word_count[word] += count\n    return album_word_count\n</pre> def get_album_word_counts(album_dir):     \"\"\"     Inputs -- str: path to album directory     Outputs -- dictionary with word counts for the entire album     \"\"\"     album_word_count = {}      for root, dirs, files in os.walk(album_dir):         for file in files:             if file.endswith('.txt'):  # Assuming song files are in .txt format                 song_path = os.path.join(root, file)                 song_list = get_song_list(song_path)                 word_counts = get_song_word_counts(song_list)                  # Aggregate word counts for the album                 for word, count in word_counts.items():                     if word not in album_word_count:                         album_word_count[word] = count                     else:                         album_word_count[word] += count     return album_word_count In\u00a0[\u00a0]: Copied! <pre>def get_whole_word_counts(data_dir):\n    \"\"\"\n    Inputs -- str: path to data directory containing album subdirectories\n    Outputs -- dictionary with album names as keys and word counts as values\n    \"\"\"\n    whole_word_counts = {}\n    \n    # Iterate over each subdirectory in the data directory\n    for album_name in os.listdir(data_dir):\n        album_path = os.path.join(data_dir, album_name)\n        if os.path.isdir(album_path):\n            print(f\"Processing album: {album_name}\")\n            album_word_counts = get_album_word_counts(album_path)\n            whole_word_counts[album_name] = album_word_counts\n    \n    return whole_word_counts\n</pre> def get_whole_word_counts(data_dir):     \"\"\"     Inputs -- str: path to data directory containing album subdirectories     Outputs -- dictionary with album names as keys and word counts as values     \"\"\"     whole_word_counts = {}          # Iterate over each subdirectory in the data directory     for album_name in os.listdir(data_dir):         album_path = os.path.join(data_dir, album_name)         if os.path.isdir(album_path):             print(f\"Processing album: {album_name}\")             album_word_counts = get_album_word_counts(album_path)             whole_word_counts[album_name] = album_word_counts          return whole_word_counts In\u00a0[\u00a0]: Copied! <pre>data_dir = 'data'  \nwhole_word_counts = get_whole_word_counts(data_dir)\n</pre> data_dir = 'data'   whole_word_counts = get_whole_word_counts(data_dir) In\u00a0[\u00a0]: Copied! <pre># Write the dictionary to a JSON file\njson_file_path = 'album_word_counts.json'\nwith open(json_file_path, 'w', encoding='utf-8') as json_file:\n    json.dump(whole_word_counts, json_file, ensure_ascii=False, indent=4)\n</pre> # Write the dictionary to a JSON file json_file_path = 'album_word_counts.json' with open(json_file_path, 'w', encoding='utf-8') as json_file:     json.dump(whole_word_counts, json_file, ensure_ascii=False, indent=4) In\u00a0[\u00a0]: Copied! <pre>print(f\"Word counts have been written to {json_file_path}.\")\n</pre> print(f\"Word counts have been written to {json_file_path}.\")"}]}